\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{mathbbol}
\usepackage{tabularx}
\usepackage{array}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{skmath}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\title{Linear Algebra}

\begin{document}
\maketitle

\paragraph[short]{Scalar:} A single numerical value. It represents magnitude without direction. For example: $a = 5$ or $b = -3.14$

\paragraph[short]{Vector:} An ordered array of numbers arranged in a single column ($n\times1$) or row ($1\times n$). It is used to represent features or quantities with direction. For example: \[X=\begin{bmatrix}10\\20\\30\end{bmatrix}\]

\paragraph[short]{Matrix:} A two-dimensional array of numbers arranged in rows and columns ($m\times n$, where $m$ is the number of rows and $n$ is the number of columns). It is used to represent multiple features, data points, or linear transformations. For example: \[A=\begin{bmatrix}1 & 2 & 3\\4 & 5 & 6\\7 & 8 & 9\end{bmatrix}\]

\paragraph[short]{Set \& Subset:}
\begin{itemize}
    \item Set $S \rightarrow$ A collection of elements, $S = \{1, 2, 3\}$
    \item Subset $A \subseteq B \rightarrow$ All elements of $A$ are in $B$, $A = \{1, 2\}$, $B = \{1, 2, 3, 4, 5\}$
    \item Universal Set $U \rightarrow$ The complete set under consideration, $U = \{1, 2, 3, 4, 5, 6, \dots\}$
    \item Empty Set $\emptyset \rightarrow$ A set without any elements.
    \item Union $A \cup B \rightarrow$ A set that contains all the elements from both the sets, $A = \{1, 2, 3\}$, $B = \{3, 4, 5\}$, $A \cup B = \{1, 2, 3, 4, 5\}$
    \item Intersection $A \cap B \rightarrow$ A set that contains only the elements that are present in both the sets, $A = \{1, 2, 3\}$, $B = \{3, 4, 5\}$, $A \cap B = \{3\}$
\end{itemize}

\paragraph[short]{Vector Space:} A set $V$ of vectors together with two operations (vector addition and scalar multiplication) that satisfy the following axioms:
\begin{enumerate}
    \item Closure under addition: $\mathbf{u} + \mathbf{v} \in V$ for all $\mathbf{u}, \mathbf{v} \in V$
    \item Closure under scalar multiplication: $c\mathbf{u} \in V$ for all $\mathbf{u} \in V$ and scalar $c$
    \item Addition is commutative: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
    \item Addition is associative: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$
    \item Existence of zero vector: There exists $\mathbf{0} \in V$ such that $\mathbf{u} + \mathbf{0} = \mathbf{u}$
    \item Existence of additive inverse: For each $\mathbf{u} \in V$, there exists $-\mathbf{u} \in V$ such that $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$
    \item Distributivity: $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$ and $(c + d)\mathbf{u} = c\mathbf{u} + d\mathbf{u}$
    \item Scalar multiplication is associative: $(cd)\mathbf{u} = c(d\mathbf{u})$
    \item Identity element: $1\mathbf{u} = \mathbf{u}$
\end{enumerate}
For example: $\mathbb{R}^3$ is a vector space consisting of all 3-dimensional vectors like $\begin{bmatrix}x\\y\\z\end{bmatrix}$ where $x, y, z \in \mathbb{R}$. If $\mathbf{u} = \begin{bmatrix}1\\2\\3\end{bmatrix}$ and $\mathbf{v} = \begin{bmatrix}4\\5\\6\end{bmatrix}$, then $\mathbf{u} + \mathbf{v} = \begin{bmatrix}5\\7\\9\end{bmatrix} \in \mathbb{R}^3$ and $2\mathbf{u} = \begin{bmatrix}2\\4\\6\end{bmatrix} \in \mathbb{R}^3$.

\paragraph[short]{Sub-space:} A subset of a vector space that is itself a vector space (it must contain the zero vector, be closed under addition, and closed under scalar multiplication). For example: In $\mathbb{R}^3$, all vectors of the form $\begin{bmatrix}x\\y\\0\end{bmatrix}$ form a sub-space ($\mathbb{R}^2$ - the xy-plane), because adding any two such vectors or multiplying by a scalar keeps the third component zero.

\paragraph[short]{Linear Independence:} A set of vectors is linearly independent if no vector can be written as a combination of the others. In other words, $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_n\mathbf{v}_n = \mathbf{0}$ only when all $c_i = 0$. For example: Vectors $\mathbf{v}_1 = \begin{bmatrix}1\\0\end{bmatrix}$ and $\mathbf{v}_2 = \begin{bmatrix}0\\1\end{bmatrix}$ are linearly independent, but $\mathbf{v}_1 = \begin{bmatrix}1\\2\end{bmatrix}$ and $\mathbf{v}_2 = \begin{bmatrix}2\\4\end{bmatrix}$ are not (since $\mathbf{v}_2 = 2\mathbf{v}_1$).

\paragraph[short]{Norm:} A measure of the length or magnitude of a vector, denoted as $\|\mathbf{v}\|$. The most common is the Euclidean norm (L2 norm): $\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}$. For example: The norm of $\mathbf{v} = \begin{bmatrix}3\\4\end{bmatrix}$ is $\|\mathbf{v}\| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = 5$. Commonly used norms:
\begin{itemize}
    \item Lp Norm (General Norm)
    \item L1 Norm (Manhattan Norm)
    \item L2 Norm (Euclidean Norm)
    \item L$\infty$ Norm (Chebyshev Distance)
\end{itemize}

\paragraph[short]{Matrix Addition \& Subtraction:} Two matrices of the same dimensions can be added or subtracted by adding or subtracting their corresponding elements. For example: If $A = \begin{bmatrix}1 & 2\\3 & 4\end{bmatrix}$ and $B = \begin{bmatrix}5 & 6\\7 & 8\end{bmatrix}$, then $A + B = \begin{bmatrix}1+5 & 2+6\\3+7 & 4+8\end{bmatrix} = \begin{bmatrix}6 & 8\\10 & 12\end{bmatrix}$ and $A - B = \begin{bmatrix}1-5 & 2-6\\3-7 & 4-8\end{bmatrix} = \begin{bmatrix}-4 & -4\\-4 & -4\end{bmatrix}$.

\begin{center}
    \begin{tabular}{|l|l|l|}
        \hline
        Property          & Addition                    & Subtraction                 \\
        \hline
        Commutative       & $A + B = B + A$             & $A + B \neq B - A$          \\
        \hline
        Associative       & $(A + B) + C = A + (B + C)$ & $A - (B - C) = (A - B) + C$ \\
        \hline
        Additive Identity & $ A + 0 = A$                & $A - 0 = A$                 \\
        \hline
        Additive Inverse  & $ A + (-A) = 0$             & $A - A = 0$                 \\
        \hline
    \end{tabular}
\end{center}

Scalar addition and subtractions are allowed as well. In that case, the scalar is added to or subtracted from all the elements. For example: $2 + A = \begin{bmatrix}
        1+2 & 2+2 \\
        3+2 & 4+2
    \end{bmatrix} = \begin{bmatrix}
        3 & 4 \\
        5 & 6
    \end{bmatrix}$.

\paragraph[short]{Trace:} The sum of all diagonal elements of a square matrix, denoted as $\text{tr}(A)$ or $\text{Tr}(A)$. For example: For matrix $A = \begin{bmatrix}1 & 2 & 3\\4 & 5 & 6\\7 & 8 & 9\end{bmatrix}$, the trace is $\text{tr}(A) = 1 + 5 + 9 = 15$.

\paragraph[short]{Matrix Multiplication:} To multiply two matrices $A$ and $B$, the number of columns in $A$ must equal the number of rows in $B$. If $A$ is $m \times n$ and $B$ is $n \times p$, the resulting matrix $C$ will be $m \times p$. Each element $C_{ij}$ is computed as the dot product of the $i$-th row of $A$ and the $j$-th column of $B$.

For example, multiplying a $3 \times 2$ matrix with a $2 \times 3$ matrix:
\[A = \begin{bmatrix}1 & 2\\3 & 4\\5 & 6\end{bmatrix}, \quad B = \begin{bmatrix}7 & 8 & 9\\10 & 11 & 12\end{bmatrix}\]

Step-by-step calculation of $C = AB$ (which will be $3 \times 3$):
\begin{align*}
    C_{11} & = (1)(7) + (2)(10) = 7 + 20 = 27   \\
    C_{12} & = (1)(8) + (2)(11) = 8 + 22 = 30   \\
    C_{13} & = (1)(9) + (2)(12) = 9 + 24 = 33   \\
    C_{21} & = (3)(7) + (4)(10) = 21 + 40 = 61  \\
    C_{22} & = (3)(8) + (4)(11) = 24 + 44 = 68  \\
    C_{23} & = (3)(9) + (4)(12) = 27 + 48 = 75  \\
    C_{31} & = (5)(7) + (6)(10) = 35 + 60 = 95  \\
    C_{32} & = (5)(8) + (6)(11) = 40 + 66 = 106 \\
    C_{33} & = (5)(9) + (6)(12) = 45 + 72 = 117
\end{align*}

Therefore: \[C = AB = \begin{bmatrix}27 & 30 & 33\\61 & 68 & 75\\95 & 106 & 117\end{bmatrix}\]

\begin{center}
    \begin{tabular}{|l|l|l|}
        \hline
        Property        & Formula              & Hold Always?   \\
        \hline
        Associative     & $(AB)C = A(BC)$      & YES            \\
        \hline
        Distributive    & $A(B+C) = (AB + AC)$ & YES            \\
        \hline
        Identity Matrix & $AI = IA = A$        & YES            \\
        \hline
        Commutative     & $AB = BA$            & \color{red} NO \\
        \hline
    \end{tabular}
\end{center}

\paragraph[short]{Diagonal Matrix:} A square matrix with non-zero elements only on the main diagonal. \[D = \begin{bmatrix}
        d_1 & 0   & 0   \\
        0   & d_2 & 0   \\
        0   & 0   & d_3 \\
    \end{bmatrix}\]

Properties:
\begin{itemize}
    \item Transpose is itself: $D^T = D$.
    \item Easy to compute inverse: \[D^{-1} = \begin{bmatrix}
                  \frac{1}{d_1} & 0             & 0             \\
                  0             & \frac{1}{d_2} & 0             \\
                  0             & 0             & \frac{1}{d_3} \\
              \end{bmatrix}\]
\end{itemize}

\paragraph[short]{Identity Matrix:} An identity matrix is a square matrix with 1s on the main diagonal and 0s everywhere else. \[I_n = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
    \end{bmatrix}\]

\pagebreak

Properties:
\begin{itemize}
    \item Multiplication Identity: $AI = IA$
    \item Inverse is itself: $I^{-1} = I$
\end{itemize}

\paragraph[short]{Transpose of a Matrix:} If a matrix $A$ has a shape of $m \times n$, then it's transpose $A^T$ (or $A'$) will have a shape of $n \times m$.
\begin{itemize}
    \item Rows become columns.
    \item Columns become row.
\end{itemize}
For example:
\[A = \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6
    \end{bmatrix}_{2 \times 3} \quad \rightarrow \quad A^T = \begin{bmatrix}
        1 & 4 \\
        2 & 5 \\
        3 & 6
    \end{bmatrix}_{3 \times 2} \]

\begin{center}
    \begin{tabularx}{\textwidth}{|m{4.5cm}|m{4cm}|X|}
        \hline
        Property                   & Mathematical Expression & Explanation                                                                          \\
        \hline
        Double Transpose           & $(A^T)^T = A$           & Taking the transpose twice returns the original matrix.                              \\
        \hline
        Transpose of a Sum         & $(A+B)^T = A^T + B^T$   & Transpose of a sum is the sum of transposes.                                         \\
        \hline
        Transpose of a Product     & $(AB)^T = B^T A^T$      & Transpose of a product reverses the order of multiplication.                         \\
        \hline
        Scalar Multiplication      & $(cA)^T = cA^T$         & The scalar factor $c$ remains unchanged when transposing.                            \\
        \hline
        Symmetric Matrix Condition & $A = A^T$               & A square matrix is symmetric if it equals to it's transpose.                         \\
        \hline
        Skew-Symmetric Matrix      & $A^T = -A$              & A matrix is skew-symmetric if it's transpose is it's negative.                       \\
        \hline
        Orthogonal Matrix          & $A^T A = I$             & A matrix is orthogonal if it's transpose is equals to it's inverse ($A^T = A^{-1}$). \\
        \hline
    \end{tabularx}
\end{center}

\paragraph[short]{Determinant:} Based on determinant of a matrix -
\begin{itemize}
    \item We can decide whether the matrix is invertible ($A^{-1}$).
    \item Whether a system of linear equations has a unique solution.
    \item It is also import for volume scaling in linear transformations.
\end{itemize}

\noindent For a $2 \times 2$ matrix $A = \begin{bmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22}
    \end{bmatrix}$, $\det(A) = (a_{11} \times a_{22}) - (a_{12} \times a_{21})$. Example:
\begin{align*}
    A       & = \begin{bmatrix}
                    2 & 3 \\
                    1 & 4
                \end{bmatrix}              \\
    \det(A) & = (2 \times 4) - (3 \times 1) \\
            & = 8 - 3 = 5
\end{align*}

\noindent For a $3 \times 3$ matrix $A = \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
    \end{bmatrix}$, $\det(A) = a_{11} \begin{vmatrix}
        a_{22} & a_{23} \\
        a_{32} & a_{33} \\
    \end{vmatrix} - a_{12} \begin{vmatrix}
        a_{21} & a_{23} \\
        a_{31} & a_{33} \\
    \end{vmatrix} + a_{13} \begin{vmatrix}
        a_{21} & a_{22} \\
        a_{31} & a_{32} \\
    \end{vmatrix}$.
Example:
\begin{align*}
    A       & = \begin{bmatrix}
                    1 & 2 & 3 \\
                    4 & 5 & 6 \\
                    7 & 8 & 9 \\
                \end{bmatrix}                                                                                                                                      \\
    \det(A) & = 1 \times \begin{vmatrix}
                             5 & 6 \\
                             8 & 9 \\
                         \end{vmatrix} - 2 \times \begin{vmatrix}
                                                      4 & 6 \\
                                                      7 & 9 \\
                                                  \end{vmatrix} + 3 \times \begin{vmatrix}
                                                                               4 & 5 \\
                                                                               7 & 8 \\
                                                                           \end{vmatrix}                                                                           \\
            & =1 \times (5 \times 9 - 6 \times 8)                                         - 2 \times (4 \times 9 - 6 \times 7) + 3 \times (4 \times 8 - 5 \times 7) \\
            & = 1 \times (45 - 48) -  2 \times (36 - 42) + 3 \times (32 - 35)                                                                                       \\
            & = -3 + 12 - 9 = 0
\end{align*}

\begin{center}
    \begin{tabularx}{\textwidth}{|p{7cm}|X|}
        \hline
        $\det(A) = 0$                                               & $\det(A) \neq 0 $    \\
        \hline
        Singular Matrix                                             & Non-singular Matrix  \\
        \hline
        Not invertible                                              & Invertible           \\
        \hline
        Row/Column vectors are linearly dependent (not orthonormal) & Linearly independent \\
        \hline
    \end{tabularx}
\end{center}

\paragraph[short]{Inverse Matrix ($2 \times 2$):} Inverse of a $2 \times 2$ matrix $A = \begin{bmatrix}
        a11 & a12 \\
        a21 & a22 \\
    \end{bmatrix}$ is $A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}
        a22  & -a12 \\
        -a21 & a11
    \end{bmatrix}$. Example:
\begin{align*}
    A       & = \begin{bmatrix}
                    2 & 3 \\
                    1 & 4
                \end{bmatrix}                                         \\
    \\
    \det(A) & = (2 \times 4) - (3 \times 1)                            \\
            & = 5                                                      \\
    \\
    A^{-1}  & = \frac{1}{5} \begin{bmatrix}
                                4  & -3 \\
                                -1 & 2
                            \end{bmatrix}                             \\
            & = \begin{bmatrix}
                    \frac{4}{5}  & -\frac{3}{5} \\
                    -\frac{1}{5} & \frac{2}{5}
                \end{bmatrix}                            \\
    \\
    AA^{-1} & = \begin{bmatrix}
                    \frac{8}{5} - \frac{3}{5} & -\frac{6}{5} + \frac{6}{5} \\
                    \frac{4}{5} - \frac{4}{5} & -\frac{3}{5} + \frac{8}{5}
                \end{bmatrix} \\
            & = \begin{bmatrix}
                    1 & 0 \\
                    0 & 1 \\
                \end{bmatrix}                                         \\
            & = I
\end{align*}

\paragraph[short]{Sparse Matrix and Dense Matrix:}
\begin{itemize}[label=--]
    \item \textbf{Sparse Matrix:} Matrix that has mostly zero elements and a few non-zero elements. Characteristics -
          \begin{itemize}[label=$\bullet$]
              \item Most elements are zero (typically, more that 50\%).
              \item Memory efficient.
              \item Specialized operations (compressed storage formats, optimized computation).
              \item Commonly found in Natural Language Processing (NLP), Recommendation Systems, Computer Vision, Anomaly Detection, Genomics \& Bioinformatics, Graph Based Data (Social Networks, Knowledge Graphs), High Dimensional Feature Spaces, etc.
          \end{itemize}

          \pagebreak

    \item \textbf{Dense Matrix:} Matrix that has mostly non-zero elements. Characteristics -
          \begin{itemize}[label=$\bullet$]
              \item Few or no zero elements.
              \item Standard storage format (2D array/matrix).
              \item Computationally expensive for large sizes.
              \item Used in general numerical computation.
          \end{itemize}
\end{itemize}

\paragraph[short]{Rank of a Matrix:} The number of linearly independent row or column. It represents the dimensions of row space or column space. Example: \[
    A = \begin{bmatrix}
        1 & 2 & 3 \\
        2 & 4 & 6 \\
        3 & 6 & 9 \\
    \end{bmatrix}
\]
\indent Step-1: Identify the  column vectors.
\begin{align*}
    C_1 = \begin{bmatrix}
              1 \\ 2 \\ 3
          \end{bmatrix}, \quad
    C_2 = \begin{bmatrix}
              2 \\ 4 \\ 6
          \end{bmatrix}, \quad
    C_3 = \begin{bmatrix}
              3 \\ 6 \\ 9
          \end{bmatrix}
\end{align*}
\indent We check if these columns are linearly independent by expressing one column as a combination of others.
\begin{itemize}[leftmargin=0.5in]
    \item Observe that $C_2 = 2C_1$ and $C_3 = 3C_1$.
    \item Since all columns are multiple of $C_1$, there is only one linearly independent column.
\end{itemize}

\smallskip

\indent Thus, the rank of $A$ is 1.

\bigskip

\indent Properties of Rank:
\begin{itemize}[leftmargin=0.5in]
    \item Row rank = Column rank.
    \item If $\det(A) \neq 0$, rank is $n$ (full rank).
    \item If $\det(A) = 0$, rank is $<n$ (rank-deficient). For this type of matrices, it is easy to get the rank by transforming the matrix into \textbf{row-echelon} form. Then, just count the number of non-zero rows.
    \item Only a zero matrix has a rank of 0.
\end{itemize}

\paragraph[short]{Variance, Covariance, Covariance Matrix, \& Correlation:}
\begin{itemize}[label=--]
    \item \textbf{Variance:} Measures how much a single variable varies from its mean. It quantifies the spread of data points.
          \begin{align*}
              \text{Var}(X) & = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2
          \end{align*}
          Example: For dataset $X = [2, 4, 6]$, mean $\bar{x} = 4$.
          \begin{align*}
              \text{Var}(X) & = \frac{1}{3}[(2-4)^2 + (4-4)^2 + (6-4)^2]          \\
                            & = \frac{1}{3}[4 + 0 + 4] = \frac{8}{3} \approx 2.67
          \end{align*}

    \item \textbf{Covariance:} A statistical measurement that quantifies the relationship between two random variables. It indicates whether they increase together (positive) or inversely (negative).
          \begin{align*}
              \text{Cov}(X, Y) & = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})
          \end{align*}
          Example: For datasets $X = [1, 2, 3]$ and $Y = [2, 4, 6]$, with $\bar{x} = 2$ and $\bar{y} = 4$.
          \begin{align*}
              \text{Cov}(X, Y) & = \frac{1}{3}[(1-2)(2-4) + (2-2)(4-4) + (3-2)(6-4)] \\
                               & = \frac{1}{3}[(-1)(-2) + 0 + (1)(2)]                \\
                               & = \frac{1}{3}[2 + 0 + 2] = \frac{4}{3} \approx 1.33
          \end{align*}

          \begin{itemize}[label=$\bullet$]
              \item $\text{Cov}(X, Y) > 0$, a positive covariance. $X$ and $Y$ tend to increase or decrease together (bidirectional). Example: Height and Weight.
              \item $\text{Cov}(X, Y) < 0$, a negative covariance. When $X$ increases, $Y$ tends to decrease, and vice versa (bidirectional). Example: Speed of a car and time to reach a destination.
              \item $\text{Cov}(X, Y) = 0$, no linear relationship. Changes in $X$ don't linearly relate to changes in $Y$. Example: Shoe size and IQ.
          \end{itemize}

    \item \textbf{Covariance Matrix:} A square matrix showing covariances between multiple variables. For variables $X_1, X_2, \ldots, X_n$, the covariance matrix is:
          \[
              \Sigma = \begin{bmatrix}
                  \text{Var}(X_1)      & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_n) \\
                  \text{Cov}(X_2, X_1) & \text{Var}(X_2)      & \cdots & \text{Cov}(X_2, X_n) \\
                  \vdots               & \vdots               & \ddots & \vdots               \\
                  \text{Cov}(X_n, X_1) & \text{Cov}(X_n, X_2) & \cdots & \text{Var}(X_n)
              \end{bmatrix}
          \]
          Example: For two variables $X = [1, 2, 3]$ and $Y = [2, 4, 6]$ with $\text{Var}(X) = \frac{2}{3}$, $\text{Var}(Y) = \frac{8}{3}$, and $\text{Cov}(X, Y) = \frac{4}{3}$:
          \[
              \Sigma = \begin{bmatrix}
                  \frac{2}{3} & \frac{4}{3} \\
                  \frac{4}{3} & \frac{8}{3}
              \end{bmatrix}
          \]

          \begin{itemize}[label=$\bullet$]
              \item $\text{Var}(X_1 \pm X_2) = \text{Var}(X_1) + \text{Var}(X_2) \pm \text{Cov}(X_1, X_2)$.
              \item $\text{Cov}(X_1 + X_2, X_3 - X_4) = \text{Cov}(X_1, X_3) + \text{Cov}(X_1, -X_4) + \text{Cov}(X_2, X_3) + \text{Cov}(X_2, -X_4)$ (here $\pm$ doesn't apply, means $-X_4 = X_4$).
          \end{itemize}

    \item \textbf{Correlation:} A normalized measure of the linear relationship between two variables, ranging from $-1$ to $1$. It is dimensionless and scale-independent.
          \begin{align*}
              \text{Corr}(X, Y) & = \frac{\text{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y}
          \end{align*}
          where $\sigma_X = \sqrt{\text{Var}(X)}$ and $\sigma_Y = \sqrt{\text{Var}(Y)}$ are standard deviations.

          Example: Using previous data $X = [1, 2, 3]$ and $Y = [2, 4, 6]$ with $\text{Var}(X) = \frac{2}{3}$, $\text{Var}(Y) = \frac{8}{3}$, and $\text{Cov}(X, Y) = \frac{4}{3}$:
          \begin{align*}
              \sigma_X          & = \sqrt{\frac{2}{3}} \approx 0.816                                              \\
              \sigma_Y          & = \sqrt{\frac{8}{3}} \approx 1.633                                              \\
              \text{Corr}(X, Y) & = \frac{\frac{4}{3}}{\sqrt{\frac{2}{3}} \cdot \sqrt{\frac{8}{3}}}               \\
                                & = \frac{\frac{4}{3}}{\sqrt{\frac{16}{9}}} = \frac{\frac{4}{3}}{\frac{4}{3}} = 1
          \end{align*}
          A correlation ($\rho$) of $1$ means perfect positive linear relationship (as one increases, the other increases proportionally).
\end{itemize}

\paragraph[short]{Eigenvalue:} Eigenvalues are special numbers associated with square matrix that indicate how the matrix scales or transforms vectors. For a matrix $A$, an eigenvalue $\lambda$ is scalar that satisfies the equation:
\[
    Av = \lambda{v} \quad \left| \quad \begin{array}{l}
        A = \text{square matrix}          \\
        v = \text{eigenvector (non-zero)} \\
        \lambda = \text{eigenvalue (scalar)}
    \end{array} \right.
\]
Since $\lambda{v}$ represents scalar multiplication of the vector $v$, $\lambda$ must be a scalar. Eigenvalues are found by solving the characteristic equation:
\[\det(A - \lambda{I}) = 0\]

\noindent Example:
\begin{align*}
    A                                      & = \begin{bmatrix}
                                                   2  & -1 \\
                                                   -1 & 2
                                               \end{bmatrix}
    \\ \\
    A - \lambda{I}                         & =  \begin{bmatrix}
                                                    2  & -1 \\
                                                    -1 & 2
                                                \end{bmatrix} - \lambda \begin{bmatrix}
                                                                            1 & 0 \\
                                                                            0 & 1
                                                                        \end{bmatrix} \\
                                           & = \begin{bmatrix}
                                                   2 - \lambda & -1          \\
                                                   -1          & 2 - \lambda
                                               \end{bmatrix}
    \\ \\
    \det(A - \lambda{I})                   & = (2 - \lambda)^2 - 1                     \\
                                           & = 4 - 4\lambda + \lambda^2 - 1            \\
                                           & = \lambda^2 - 4\lambda + 3
    \\ \\
    \det(A - \lambda{I})                   & = 0                                       \\
    \lambda^2 - 4\lambda + 3               & = 0                                       \\
    \lambda^2 - 3\lambda - \lambda + 3     & = 0                                       \\
    \lambda(\lambda -  3) - 1(\lambda - 3) & = 0                                       \\
    (\lambda - 3)(\lambda - 1)             & = 0                                       \\
    \lambda                                & = 3, 1
\end{align*}

\noindent The sign of eigenvalues determines the definiteness of a symmetric matrix $A$, which relates to the quadratic form $x^T A x$ for all non-zero vectors $x$:
\begin{itemize}
    \item \textbf{Positive Definite:} All eigenvalues $\lambda > 0$ $\Rightarrow$ $x^T A x > 0$ for all $x \neq 0$.
    \item \textbf{Negative Definite:} All eigenvalues $\lambda < 0$ $\Rightarrow$ $x^T A x < 0$ for all $x \neq 0$.
    \item \textbf{Positive Semi-Definite:} All eigenvalues $\lambda \geq 0$ $\Rightarrow$ $x^T A x \geq 0$ for all $x \neq 0$.
    \item \textbf{Negative Semi-Definite:} All eigenvalues $\lambda \leq 0$ $\Rightarrow$ $x^T A x \leq 0$ for all $x \neq 0$.
    \item \textbf{Indefinite:} Mixed positive and negative eigenvalues $\Rightarrow$ $x^T A x$ can be positive, negative, or zero. Occurs in \textbf{saddle-point} problems and optimization.
\end{itemize}

\paragraph[short]{Convexity:}
\begin{itemize}
    \item \textbf{Jensen's Inequality:}
          \[f(\lambda{x} + (1 - \lambda)y) \leq \lambda f(x) + (1 - \lambda)f(y), \quad \forall x, y \text{ and } \lambda \in [0, 1] \]
          Example:
          \begin{align*}
              f(x)           & = x^2
              \\ \\
              Assume,                                                         \\
              x = 1, y       & = 3, \text{ and } \lambda = 0.5
              \\ \\
              LHS            & = f(\lambda{x} + (1 - \lambda)y)               \\
                             & = f(0.5(1) + (1 - 0.5)3)                       \\
                             & = f(0.5 + 1.5)                                 \\
                             & = f(2)                                         \\
                             & = 2^2 = 4
              \\ \\
              RHS            & = \lambda f(x) + (1 - \lambda)f(y)             \\
                             & = 0.5(1^2) + (1 - 0.5)(3^2)                    \\
                             & = 0.5 + 4.5 = 5
              \\ \\
              LHS      < RHS & \text{, proving that } f(x) \text{ is convex.}
          \end{align*}

    \item \textbf{Hessain Matrix:} A square-symmetric matrix of second order partial derivatives of scalar-valued function. It describes the local curvature of a function and is widely used in optimization, machine learning and numerical analysis.

          \[
              H(f) = \begin{bmatrix}
                  \frac{\partial^2 f}{\partial{x^2}}          & \frac{\partial^2 f}{\partial{x}\partial{y}} & \frac{\partial^2 f}{\partial{x}\partial{z}}
                  \\[0.6em]
                  \frac{\partial^2 f}{\partial{y}\partial{x}} & \frac{\partial^2 f}{\partial{y^2}}          & \frac{\partial^2 f}{\partial{y}\partial{z}}
                  \\[0.6em]
                  \frac{\partial^2 f}{\partial{z}\partial{x}} & \frac{\partial^2 f}{\partial{z}\partial{y}} & \frac{\partial^2 f}{\partial{z^2}}
              \end{bmatrix}
              \quad
              H(f) = \begin{bmatrix}
                  \frac{\partial^2 f}{\partial{x_1}^2}            & \frac{\partial^2 f}{\partial{x_1}\partial{x_2}} & \cdots & \frac{\partial^2 f}{\partial{x_1}\partial{x_n}}
                  \\[0.6em]
                  \frac{\partial^2 f}{\partial{x_2}\partial{x_1}} & \frac{\partial^2 f}{\partial{x_2}^2}            & \cdots & \frac{\partial^2 f}{\partial{x_2}\partial{x_n}}
                  \\[0.6em]
                  \cdots                                          & \cdots                                          & \ddots & \cdots
                  \\[0.6em]
                  \frac{\partial^2 f}{\partial{x_n}\partial{x_1}} & \frac{\partial^2 f}{\partial{x_n}\partial{x_2}} & \cdots & \frac{\partial^2 f}{\partial{x_n}^2}
              \end{bmatrix}
          \]

          \begin{center}
              \begin{tabularx}{\linewidth}{|P{.2\linewidth}|P{.15\linewidth}|P{.3\linewidth}|Y|}
                  \hline
                  Hessain Type           & Eigenvalues $\lambda$ & Local Min/Max                                   & Convexity                                  \\
                  \hline
                  Positive Definite      & All $\lambda > 0$     & Local Minimum                                   & Strictly Convex                            \\
                  \hline
                  Negative Definite      & All $\lambda < 0$     & Local Maximum                                   & Strictly Concave                           \\
                  \hline
                  Positive Semi-Definite & All $\lambda \ge 0$   & Possibly Local Minimum (Need Higher-Order Test) & Convex (Possibly flat in some directions)  \\
                  \hline
                  Negative Semi-Definite & All $\lambda \le 0$   & Possibly Local Maximum (Need Higher-Order Test) & Concave (Possibly flat in some directions) \\
                  \hline
                  Indefinite             & Mixed $(\pm)$         & Saddle-Point (Neither Max nor Min)              & Strictly Convex                            \\
                  \hline
              \end{tabularx}
          \end{center}

          Example:
          \begin{align*}
              f(x, y)                                                                                                                                                       & = x^2 + y^2                            \\
              \frac{\partial^2 f}{\partial x^2} = \frac{\partial}{\partial x} (\frac{\partial f}{\partial x})                                                               & = \frac{\partial}{\partial x} (2x) = 2 \\
              \frac{\partial^2 f}{\partial y^2} = \frac{\partial}{\partial y} (\frac{\partial f}{\partial y})                                                               & = \frac{\partial}{\partial y} (2y) = 2 \\
              \frac{\partial^2 f}{\partial x \partial y}                                                      = \frac{\partial}{\partial y} (\frac{\partial f}{\partial x}) & = \frac{\partial}{\partial y} (2x) = 0 \\
          \end{align*}
          \begin{align*}
              H(f)                                   & = \begin{bmatrix}
                                                             2 & 0 \\
                                                             0 & 2
                                                         \end{bmatrix}              \\
              \det(H - \lambda I)                    & = 0                           \\
              \Rightarrow \begin{vmatrix}
                              2 - \lambda & 0           \\
                              0           & 2 - \lambda
                          \end{vmatrix}           & = 0                              \\
              \Rightarrow (2 - \lambda)(2 - \lambda) & = 0                           \\
              \Rightarrow \lambda                    & = 2                           \\
              \text{Since } f(x, y) > 0,             & \text{ it's strictly convex.}
          \end{align*}
\end{itemize}

\paragraph[short]{System of Linear Equations:}
\begin{itemize}
    \item \textbf{Linear Equation:} Algebraic equation of a line where all the variables have a maximum exponent of 1. Example: $2x + 3y = 6$.
    \item \textbf{System of Linear Equations:} A set of two or more linear equations involving the same variables. Example:  \[
              \left\{ \begin{array}{l}
                  2x + 3y = 6 \\
                  x - y = 4
              \end{array}
              \right.
          \]
          \begin{itemize}
              \item \textbf{Homogeneous System:} A system of linear equations where all the constant terms are 0. Example: \[
                        \text{General Form} \left\{
                        \begin{array}{l}
                            a_1x + b_1y + c1_z = 0 \\
                            a_2x + b_2y + c2_z = 0 \\
                            a_3x + b_3y + c3_z = 0 \\
                            \vdots
                        \end{array}
                        \right. \quad
                        \begin{array}{c}
                            \text{Three Variable} \\
                            \text{Example}
                        \end{array} \left\{
                        \begin{array}{l}
                            x + y + z = 0  \\
                            2x - y + z = 0 \\
                            x + 2y - 3z = 0
                        \end{array}
                        \right.
                    \]
          \end{itemize}
\end{itemize}

\noindent Types of Solution:
\begin{itemize}
    \item \textbf{Unique Solution:} The equations intersect at a single point (consistent and independent). Lines have different slopes.
    \item \textbf{No Solution:} The equations represent parallel lines that never intersect (inconsistent). Lines have the same slope but different y-intercepts.
    \item \textbf{Infinitely Many Solutions:} The equations represent the same line (consistent and dependent). All coefficients are proportional.
\end{itemize}

\noindent Commonly used methods to solve these systems:
\begin{itemize}
    \item \textbf{Substitution Method:} Solve one equation for one variable and substitute into the other equation. Example:
          \begin{align*}
              \text{Given:}                                  \\
              x + 2y         & = 3 \quad\dots(1)             \\
              2x + 3y        & = 6 \quad\dots(2)
              \\ \\
              \text{From equation (1), solve for } x\text{:} \\
              x              & = 3 - 2y \quad\dots(3)        \\
              \\
              \text{Substitute (3) into equation (2):}       \\
              2(3 - 2y) + 3y & = 6                           \\
              6 - 4y + 3y    & = 6                           \\
              -y             & = 0                           \\
              y              & = 0                           \\
          \end{align*}

          \begin{align*}
              \text{Substitute } y = 0 \text{ into equation (3):} \\
              x                            & = 3 - 2(0)           \\
              x                            & = 3
              \\ \\
              \therefore \text{Solution: } & (x, y) = (3, 0)
          \end{align*}

    \item \textbf{Cramer's Rule:} A method for solving systems of linear equations using determinants. For a system $Ax = b$, each variable is computed as the ratio of two determinants: $x_i = \frac{D_i}{D}$, where $D = \det(A)$ and $D_i$ is the determinant of the matrix formed by replacing the $i$-th column of $A$ with the constant vector $b$. Example:
          \begin{align*}
              \text{Given:}                                                                                \\
              x + y + z                                               & = \hspace{0.19cm}6 \quad \dots(1)  \\
              2x + 3y + z                                             & = 14 \quad \dots(2)                \\
              x + 2y + 3z                                             & = 14 \quad \dots(3)                \\ \\
              \text{Coefficient Matrix:}               \hspace{0.5cm} & \hspace{0.5cm}
              \text{Constant Vector:}                                                                      \\
              A                          = \begin{bmatrix}
                                               1 & 1 & 1 \\
                                               2 & 3 & 1 \\
                                               1 & 2 & 3
                                           \end{bmatrix}
              \hspace{0.5cm}                                          & \hspace{0.5cm}
              b = \begin{bmatrix}
                      6  \\
                      14 \\
                      14
                  \end{bmatrix}
              \\ \\
              D = \det(A)                                             & = 7 - 5 + 1                        \\
                                                                      & = 3                                \\ \\
              D_x                                                     & = \begin{vmatrix}
                                                                              6  & 1 & 1 \\
                                                                              14 & 3 & 1 \\
                                                                              14 & 2 & 3
                                                                          \end{vmatrix}                   \\
                                                                      & = 42 - 28 - 14                     \\
                                                                      & = 0
              \\ \\
              D_y                                                     & = \begin{vmatrix}
                                                                              1 & 6  & 1 \\
                                                                              2 & 14 & 1 \\
                                                                              1 & 14 & 3
                                                                          \end{vmatrix}                   \\
                                                                      & = 28 - 30 + 14                     \\
                                                                      & = 12
              \\ \\
              D_z                                                     & = \begin{vmatrix}
                                                                              1 & 1 & 6  \\
                                                                              2 & 3 & 14 \\
                                                                              1 & 2 & 14
                                                                          \end{vmatrix}                   \\
                                                                      & = 14 - 14 + 6                      \\
                                                                      & = 6                                \\
              \\
              x                                                       & = \frac{D_x}{D} = \frac{0}{3} = 0  \\
              y                                                       & = \frac{D_y}{D} = \frac{12}{3} = 4 \\
              z                                                       & = \frac{D_z}{D} = \frac{6}{3} = 2
          \end{align*}
          \begin{align*}
              \\
              \therefore \text{Solution: } & (x, y, z) = (0, 4, 2)
          \end{align*}

          When $\det(A) = 0$,
          \begin{itemize}
              \item \textbf{Case 1: Infinitely Many Solutions}
                    \begin{itemize}
                        \item All $D_i$ ($D_x, D_y, D_z, \dots$) are 0.
                        \item This means the equations are dependent (e.g., one is a combination of others), and the system has infinitely many solutions (like a plane of solutions).
                    \end{itemize}
              \item \textbf{Case 2: No Solution}
                    \begin{itemize}
                        \item At least one of $D_i$ is 0.
                        \item This means the equations contradict each other (e.g., parallel planes that never intersect), and the system is inconsistent.
                    \end{itemize}
          \end{itemize}

    \item \textbf{Gaussian Elimination:} A systematic method to transform a system of linear equations into row echelon form (upper triangular matrix) by eliminating coefficients below the diagonal using elementary row operations. The solution is then obtained through back-substitution. Example:
          \begin{align*}
              \text{Given:}                                                     \\
              x + y + z                     & = \hspace{0.19cm}6 \quad \dots(1) \\
              2x + 3y + z                   & = 14 \quad \dots(2)               \\
              x + 2y + 3z                   & = 14 \quad \dots(3)               \\ \\
              \text{Augmented Matrix:}                                          \\
              A|b                           & = \begin{bmatrix}[ccc|c]
                                                    1 & 1 & 1 & 6  \\
                                                    2 & 3 & 1 & 14 \\
                                                    1 & 2 & 3 & 14
                                                \end{bmatrix}
              \\ \\
              \text{Step 1: Eliminate first column below diagonal}              \\
              R_2'                          & = R_2 - 2R_1                      \\
              R_3'                          & = R_3 - R_1
              \\ \\
              A|b                           & = \begin{bmatrix}[ccc|c]
                                                    1 & 1 & 1  & 6 \\
                                                    0 & 1 & -1 & 2 \\
                                                    0 & 1 & 2  & 8
                                                \end{bmatrix}
              \\ \\
              \text{Step 2: Eliminate second column below diagonal}             \\
              R_3''                         & = R_3' - R_2'
              \\ \\
              A|b                           & = \begin{bmatrix}[ccc|c]
                                                    1 & 1 & 1  & 6 \\
                                                    0 & 1 & -1 & 2 \\
                                                    0 & 0 & 3  & 6
                                                \end{bmatrix}
              \\ \\
              \text{Back-substitution:}                                         \\
              \text{From row 3: } 3z        & = 6 \Rightarrow z = 2             \\
              \text{From row 2: } y - z     & = 2 \Rightarrow y = 2 + z = 4     \\
              \text{From row 1: } x + y + z & = 6 \Rightarrow x = 6 - y - z = 0 \\
              \\
              \therefore \text{Solution: }  & (x, y, z) = (0, 4, 2)
          \end{align*}

          \pagebreak

          \textbf{Elementary Row Operations (Constraints):}
          \begin{itemize}
              \item \textbf{Row Replacement:} Replace any row with a linear combination of rows. Example: $R_2' = R_2 + 3R_1 - 2R_3$.
              \item \textbf{Row Swap:} Interchange any two rows. Example: $R_1 \leftrightarrow R_3$.
              \item \textbf{Row Scaling:} Multiply a row by any \textbf{non-zero} scalar. Example: $R_2' = 5R_2$.
              \item \textbf{Not Allowed:} Multiplying a row by zero (destroys information and makes the system unsolvable).
          \end{itemize}

          These operations preserve the solution set of the system (equivalent systems).
\end{itemize}

\paragraph[short]{Inverse Matrix ($3 \times 3$):} The inverse of a $3 \times 3$ matrix can be found using \textbf{elementary row operations} on the augmented matrix $[A|I]$. Transform the left side to $I$ and the right side becomes $A^{-1}$. Example:
\begin{align*}
    A                       & = \begin{bmatrix}
                                    1  & 2  & -1 \\
                                    -2 & 0  & 1  \\
                                    1  & -1 & 0
                                \end{bmatrix}           \\
    \\
    \text{Goal: } AA^{-1}   & = I                        \\
    \text{Augmented Matrix} & = [A|I]                    \\
                            & = \begin{bmatrix}[ccc|ccc]
                                    1  & 2  & -1 & 1 & 0 & 0 \\
                                    -2 & 0  & 1  & 0 & 1 & 0 \\
                                    1  & -1 & 0  & 0 & 0 & 1
                                \end{bmatrix} \\
    \\
    R_2'                    & = R_2 + 2R_1               \\
    R_3'                    & = R_3 - R_1                \\
    \\
    A|I                     & = \begin{bmatrix}[ccc|ccc]
                                    1 & 2  & -1 & 1  & 0 & 0 \\
                                    0 & 4  & -1 & 2  & 1 & 0 \\
                                    0 & -3 & 1  & -1 & 0 & 1
                                \end{bmatrix} \\
    \\
    R_3''                   & = 4R_3' + 3R_2'            \\
    \\
    A|I                     & = \begin{bmatrix}[ccc|ccc]
                                    1 & 2 & -1 & 1 & 0 & 0 \\
                                    0 & 4 & -1 & 2 & 1 & 0 \\
                                    0 & 0 & 1  & 2 & 3 & 4
                                \end{bmatrix}  \\
    \\
    R_2'''                  & = R_2' + R_3''             \\
    \\
    A|I                     & = \begin{bmatrix}[ccc|ccc]
                                    1 & 2 & 0 & 3 & 3 & 4 \\
                                    0 & 4 & 0 & 4 & 4 & 4 \\
                                    0 & 0 & 1 & 2 & 3 & 4
                                \end{bmatrix}  \\
    \\
    R_2^{(4)}               & = \frac{1}{4}R_2'''        \\
    \\
    A|I                     & = \begin{bmatrix}[ccc|ccc]
                                    1 & 2 & 0 & 3 & 3 & 4 \\
                                    0 & 1 & 0 & 1 & 1 & 1 \\
                                    0 & 0 & 1 & 2 & 3 & 4
                                \end{bmatrix}  \\
\end{align*}

\begin{align*}
    R_1'              & = R_1 - 2R_2^{(4)}         \\
    \\
    A|I               & = \begin{bmatrix}[ccc|ccc]
                              1 & 0 & 0 & 1 & 1 & 2 \\
                              0 & 1 & 0 & 1 & 1 & 1 \\
                              0 & 0 & 1 & 2 & 3 & 4
                          \end{bmatrix}  \\
    \\
    \therefore A^{-1} & = \begin{bmatrix}
                              1 & 1 & 2 \\
                              1 & 1 & 1 \\
                              2 & 3 & 4
                          \end{bmatrix}
    \\ \\
    \text{Verification:}                           \\
    AA^{-1}           & = \begin{bmatrix}
                              1  & 2  & -1 \\
                              -2 & 0  & 1  \\
                              1  & -1 & 0
                          \end{bmatrix}
    \begin{bmatrix}
        1 & 1 & 2 \\
        1 & 1 & 1 \\
        2 & 3 & 4
    \end{bmatrix}                                 \\
                      & = \begin{bmatrix}
                              1+2-2  & 1+2-3  & 2+2-4  \\
                              -2+0+2 & -2+0+3 & -4+0+4 \\
                              1-1+0  & 1-1+0  & 2-1+0
                          \end{bmatrix} \\
                      & = \begin{bmatrix}
                              1 & 0 & 0 \\
                              0 & 1 & 0 \\
                              0 & 0 & 1
                          \end{bmatrix} = I
\end{align*}

\paragraph[short]{LU Decomposition:} A method of factorizing a square matrix $A$ into the product of a lower triangular matrix $L$ and an upper triangular matrix $U$, such that $A = LU$. This decomposition is particularly useful for:
\begin{itemize}
    \item Efficiently solving multiple systems of linear equations with the same coefficient matrix
    \item Computing determinants: $\det(A) = \det(L) \times \det(U)$
    \item Finding matrix inverses
    \item Numerical stability in computations
\end{itemize}

\noindent The decomposition process uses Gaussian Elimination without row swaps (or with partial pivoting if needed). The matrix $L$ contains the multipliers used during elimination, and $U$ is the resulting upper triangular matrix.

\noindent Example: Find the LU decomposition of matrix $A$ and use it to solve $Ax = b$.
\begin{align*}
    A                        = \begin{bmatrix}
                                   2 & 1 & 1 \\
                                   4 & 3 & 3 \\
                                   8 & 7 & 9
                               \end{bmatrix}, & \quad
    b = \begin{bmatrix}
            4  \\
            10 \\
            24
        \end{bmatrix}
    \\ \\
    \text{Step 1: Perform Gaussian Elimination to get } U \text{ and record multipliers for } L
    \\ \\
    \text{Original matrix: }                   & A = \begin{bmatrix}
                                                         2 & 1 & 1 \\
                                                         4 & 3 & 3 \\
                                                         8 & 7 & 9
                                                     \end{bmatrix}
\end{align*}

\begin{align*}
    \text{Eliminate first column: }                             &                                        \\
    m_{21} = \frac{4}{2} = 2, \quad                             & m_{31} = \frac{8}{2} = 4               \\
    R_2' = R_2 - 2R_1, \quad                                    & R_3' = R_3 - 4R_1
    \\ \\
                                                                & \begin{bmatrix}
                                                                      2 & 1 & 1 \\
                                                                      0 & 1 & 1 \\
                                                                      0 & 3 & 5
                                                                  \end{bmatrix}
    \\
    \text{Eliminate second column: }                            &                                        \\
    m_{32} = \frac{3}{1} = 3                                    &                                        \\
    R_3'' = R_3' - 3R_2'                                        &
    \\ \\
    U                                                           & = \begin{bmatrix}
                                                                        2 & 1 & 1 \\
                                                                        0 & 1 & 1 \\
                                                                        0 & 0 & 2
                                                                    \end{bmatrix}
    \\
    \text{Step 2: Construct } L \text{ using the multipliers}
    \\ \\
    L                                                           & = \begin{bmatrix}
                                                                        1      & 0      & 0 \\
                                                                        m_{21} & 1      & 0 \\
                                                                        m_{31} & m_{32} & 1
                                                                    \end{bmatrix} = \begin{bmatrix}
                                                                                        1 & 0 & 0 \\
                                                                                        2 & 1 & 0 \\
                                                                                        4 & 3 & 1
                                                                                    \end{bmatrix}
    \\ \\
    \text{Verification: } LU                                    & = \begin{bmatrix}
                                                                        1 & 0 & 0 \\
                                                                        2 & 1 & 0 \\
                                                                        4 & 3 & 1
                                                                    \end{bmatrix}
    \begin{bmatrix}
        2 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 2
    \end{bmatrix}
    \\ \\
                                                                & = \begin{bmatrix}
                                                                        2 & 1 & 1 \\
                                                                        4 & 3 & 3 \\
                                                                        8 & 7 & 9
                                                                    \end{bmatrix} = A
    \\
    \text{Step 3: Solve } Ax = b \text{ using } A = LU
    \\ \\
    \text{Since } Ax = b \text{ and } A = LU,                   & \text{ we have } LUx = b               \\
    \text{Let } Ux = y, \text{ then } Ly = b                    &
    \\ \\
    \text{First, solve } Ly = b \text{ (forward substitution):} &
    \\ \\
    \begin{bmatrix}
        1 & 0 & 0 \\
        2 & 1 & 0 \\
        4 & 3 & 1
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\ y_2 \\ y_3
    \end{bmatrix}                                           & = \begin{bmatrix}
                                                                    4 \\ 10 \\ 24
                                                                \end{bmatrix}                           \\                       \\
    y_1                                                         & = 4                                    \\
    2y_1 + y_2                                                  & = 10 \Rightarrow y_2 = 10 - 8 = 2      \\
    4y_1 + 3y_2 + y_3                                           & = 24 \Rightarrow y_3 = 24 - 16 - 6 = 2
    \\ \\
    \therefore y                                                & = \begin{bmatrix}
                                                                        4 \\ 2 \\ 2
                                                                    \end{bmatrix}
\end{align*}

\begin{align*}
    \text{Then, solve } Ux = y \text{ (back-substitution):} &
    \\ \\
    \begin{bmatrix}
        2 & 1 & 1 \\
        0 & 1 & 1 \\
        0 & 0 & 2
    \end{bmatrix}
    \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix}                                       & = \begin{bmatrix}
                                                                4 \\ 2 \\ 2
                                                            \end{bmatrix}
    \\ \\
    2x_3                                                    & = 2 \Rightarrow x_3 = 1                                  \\
    x_2 + x_3                                               & = 2 \Rightarrow x_2 = 2 - 1 = 1                          \\
    2x_1 + x_2 + x_3                                        & = 4 \Rightarrow 2x_1 = 4 - 1 - 1 = 2 \Rightarrow x_1 = 1
    \\ \\
    \therefore \text{Solution: }                            & (x_1, x_2, x_3) = (1, 1, 1)
\end{align*}

\textbf{Edge Cases and Constraints in LU Decomposition:}
\begin{itemize}
    \item \textbf{Standard LU Form:} The basic LU decomposition assumes row operations of the form $R_i' = R_i - m_{ij}R_j$, where $m_{ij} = \frac{a_{ij}}{a_{jj}}$ is the multiplier. This requires no row scaling before elimination.

    \item \textbf{When Row Scaling is Needed:} If you need to perform $R_2' = 2R_2 - 3R_1$, this indicates you're scaling row 2 by a factor of 2 before elimination. To convert this to standard LU form, we factor out the scaling into a diagonal matrix $D$, giving $A = LDU$.

          Example: Solve $Ax = b$ where $A = \begin{bmatrix}
                  2 & 4 \\
                  3 & 7
              \end{bmatrix}$ and $b = \begin{bmatrix}
                  6 \\
                  13
              \end{bmatrix}$, using $R_2' = 2R_2 - 3R_1$:
          \begin{align*}
              R_2'                                                   & = 2\begin{bmatrix}3 & 7\end{bmatrix} - 3\begin{bmatrix}2 & 4\end{bmatrix} = \begin{bmatrix}0 & 2\end{bmatrix}
              \\ \\
              \text{Matrix after operation: }                        & \begin{bmatrix}
                                                                           2 & 4 \\
                                                                           0 & 2
                                                                       \end{bmatrix}
              \\ \\
              \text{Since } R_2' = 2R_2 - 3R_1\text{, factor out 2:} &                                                                                                               \\
              R_2'                                                   & = 2(R_2 - \frac{3}{2}R_1)
              \\ \\
              \text{This means row 2 was scaled by 2.}                                                                                                                               \\
              \text{To extract this scaling:}                                                                                                                                        \\
              L                                                      & = \begin{bmatrix}
                                                                             1           & 0 \\
                                                                             \frac{3}{2} & 1
                                                                         \end{bmatrix} \text{ (multiplier is } \frac{3}{2}\text{)}
              \\ \\
              D                                                      & = \begin{bmatrix}
                                                                             1 & 0 \\
                                                                             0 & 2
                                                                         \end{bmatrix} \text{ (scaling factor from } 2R_2\text{)}
              \\ \\
              U                                                      & = \begin{bmatrix}
                                                                             2 & 4 \\
                                                                             0 & 1
                                                                         \end{bmatrix} \text{ (normalized row 2: } \frac{R_2'}{2}\text{)}
              \\ \\
              \text{Verification: } A                                & = LDU                                                                                                         \\
                                                                     & = \begin{bmatrix}
                                                                             1           & 0 \\
                                                                             \frac{3}{2} & 1
                                                                         \end{bmatrix}
              \begin{bmatrix}
                  1 & 0 \\
                  0 & 2
              \end{bmatrix}
              \begin{bmatrix}
                  2 & 4 \\
                  0 & 1
              \end{bmatrix}
          \end{align*}

          \begin{align*}
                                                                      & = \begin{bmatrix}
                                                                              1           & 0 \\
                                                                              \frac{3}{2} & 1
                                                                          \end{bmatrix}
              \begin{bmatrix}
                  2 & 4 \\
                  0 & 2
              \end{bmatrix} = \begin{bmatrix}
                                  2 & 4 \\
                                  3 & 7
                              \end{bmatrix} = A
              \\ \\
              \text{To solve } Ax = b \text{ using } LDUx = b\text{:} &
              \\ \\
              \text{Step 1: Solve } Ly = b                            & \text{ (forward substitution)}
              \\ \\
              \begin{bmatrix}
                  1           & 0 \\
                  \frac{3}{2} & 1
              \end{bmatrix}
              \begin{bmatrix}
                  y_1 \\ y_2
              \end{bmatrix}                                         & = \begin{bmatrix}
                                                                            6 \\
                                                                            13
                                                                        \end{bmatrix}
              \\ \\
              y_1                                                     & = 6                                                \\
              \frac{3}{2}y_1 + y_2                                    & = 13 \Rightarrow y_2 = 13 - 9 = 4
              \\ \\
              \text{Step 2: Solve } DUx = y                           & \text{ (let } z = Ux\text{, solve } Dz = y\text{)}
              \\ \\
              \begin{bmatrix}
                  1 & 0 \\
                  0 & 2
              \end{bmatrix}
              \begin{bmatrix}
                  z_1 \\ z_2
              \end{bmatrix}                                         & = \begin{bmatrix}
                                                                            6 \\
                                                                            4
                                                                        \end{bmatrix}
              \\ \\
              z_1                                                     & = 6                                                \\
              2z_2                                                    & = 4 \Rightarrow z_2 = 2
              \\ \\
              \text{Step 3: Solve } Ux = z                            & \text{ (back-substitution)}
              \\ \\
              \begin{bmatrix}
                  2 & 4 \\
                  0 & 1
              \end{bmatrix}
              \begin{bmatrix}
                  x_1 \\ x_2
              \end{bmatrix}                                         & = \begin{bmatrix}
                                                                            6 \\
                                                                            2
                                                                        \end{bmatrix}
              \\ \\
              x_2                                                     & = 2                                                \\
              2x_1 + 4x_2                                             & = 6 \Rightarrow x_1 = \frac{6-8}{2} = -1
              \\ \\
              \therefore \text{Solution: }                            & (x_1, x_2) = (-1, 2)
          \end{align*}

    \item \textbf{Zero Pivot Problem:} If a diagonal element (pivot) is zero, standard LU decomposition fails. Solutions:
          \begin{itemize}
              \item \textbf{Row Swapping (Partial Pivoting):} Leads to PLU decomposition where $PA = LU$ and $P$ is a permutation matrix. Example: Decompose $A = \begin{bmatrix}
                            0 & 1 & 2 \\
                            2 & 3 & 4 \\
                            4 & 5 & 6
                        \end{bmatrix}$ (note: $a_{11} = 0$)

                    \begin{align*}
                        \text{Cannot proceed with standard LU since pivot } a_{11} = 0                      \\
                        \text{Swap } R_1 \leftrightarrow R_2 \text{ to get largest pivot:} &
                        \\ \\
                        PA                                                                 = \begin{bmatrix}
                                                                                                 0 & 1 & 0 \\
                                                                                                 1 & 0 & 0 \\
                                                                                                 0 & 0 & 1
                                                                                             \end{bmatrix}
                        \begin{bmatrix}
                            0 & 1 & 2 \\
                            2 & 3 & 4 \\
                            4 & 5 & 6
                        \end{bmatrix}                                                    & = \begin{bmatrix}
                                                                                                 2 & 3 & 4 \\
                                                                                                 0 & 1 & 2 \\
                                                                                                 4 & 5 & 6
                                                                                             \end{bmatrix}
                    \end{align*}

                    \begin{align*}
                        \text{Now perform LU on } PA:           &
                        \\ \\
                        m_{31} = \frac{4}{2} = 2, \quad R_3'    & = R_3 - 2R_1
                        \\ \\
                                                                & \begin{bmatrix}
                                                                      2 & 3  & 4  \\
                                                                      0 & 1  & 2  \\
                                                                      0 & -1 & -2
                                                                  \end{bmatrix}
                        \\ \\
                        m_{32} = \frac{-1}{1} = -1, \quad R_3'' & = R_3' - (-1)R_2 = R_3' + R_2
                        \\ \\
                        U                                       & = \begin{bmatrix}
                                                                        2 & 3 & 4 \\
                                                                        0 & 1 & 2 \\
                                                                        0 & 0 & 0
                                                                    \end{bmatrix}
                        \\ \\
                        L                                       & = \begin{bmatrix}
                                                                        1 & 0  & 0 \\
                                                                        0 & 1  & 0 \\
                                                                        2 & -1 & 1
                                                                    \end{bmatrix}
                        \\ \\
                        \therefore PA                           & = LU \text{ (PLU decomposition)}
                    \end{align*}
          \end{itemize}
    \item \textbf{Singular Matrix:} If $\det(A) = 0$, the matrix has no unique LU decomposition (or the decomposition exists but $U$ will have a zero on the diagonal, making the system unsolvable or having infinitely many solutions).

          Example: $A = \begin{bmatrix}
                  1 & 2 \\
                  2 & 4
              \end{bmatrix}$ (note: row 2 is 2 times row 1)
          \begin{align*}
              \det(A)                                       & = 1 \times 4 - 2 \times 2 = 0
              \\ \\
              \text{Attempt LU:}                            &
              \\ \\
              m_{21} = \frac{2}{1} = 2, \quad R_2'          & = R_2 - 2R_1
              \\ \\
              U                                             & = \begin{bmatrix}
                                                                    1 & 2 \\
                                                                    0 & 0
                                                                \end{bmatrix}, \quad
              L = \begin{bmatrix}
                      1 & 0 \\
                      2 & 1
                  \end{bmatrix}
              \\ \\
              \text{The zero in } U_{22} \text{ indicates } & \text{the matrix is singular.}
              \\ \\
              \text{If solving } Ax = b:                    &
              \\ \\
              \text{- If } b                                & \text{ is consistent with the dependency, infinitely many solutions}
              \\
              \text{- If } b                                & \text{ is inconsistent, no solution}
          \end{align*}

    \item \textbf{Non-Square Matrices:} LU decomposition can be extended to rectangular matrices ($m \times n$), but $L$ will be $m \times m$ and $U$ will be $m \times n$ (or vice versa depending on the variant).

          Example: Decompose $A = \begin{bmatrix}
                  2 & 1 & 3 \\
                  4 & 3 & 5
              \end{bmatrix}$ (23 matrix)
          \begin{align*}
              m_{21} = \frac{4}{2} = 2, \quad R_2' & = R_2 - 2R_1
              \\ \\
              U                                    & = \begin{bmatrix}
                                                           2 & 1 & 3  \\
                                                           0 & 1 & -1
                                                       \end{bmatrix}_{2 \times 3}, \quad
              L = \begin{bmatrix}
                      1 & 0 \\
                      2 & 1
                  \end{bmatrix}_{2 \times 2}
              \\ \\
              \text{Verification: } LU             & = \begin{bmatrix}
                                                           1 & 0 \\
                                                           2 & 1
                                                       \end{bmatrix}
              \begin{bmatrix}
                  2 & 1 & 3  \\
                  0 & 1 & -1
              \end{bmatrix}
              \\ \\
                                                   & = \begin{bmatrix}
                                                           2 & 1 & 3 \\
                                                           4 & 3 & 5
                                                       \end{bmatrix} = A
          \end{align*}

    \item \textbf{Best Practice:} For numerical stability, always use \textbf{partial pivoting} (swap rows to get the largest pivot), resulting in PLU decomposition. This minimizes rounding errors in computer implementations.
\end{itemize}

\paragraph[short]{Calculus:}
\begin{itemize}
    \item \textbf{Differential Calculus:} The branch of calculus that deals with the study of rates of change and slopes of curves. It focuses on finding derivatives, which measure how a function changes as its input changes. The derivative of a function $f(x)$ at a point $x$ is defined as:
          \[
              f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
          \]
          This represents the instantaneous rate of change of $f$ with respect to $x$.

          \textbf{Common Differentiation Rules:}
          \begin{enumerate}
              \item \textbf{Constant Rule:} If $f(x) = c$ (where $c$ is a constant), then $f'(x) = 0$.

                    Example: If $f(x) = 7$, then $f'(x) = 0$.

              \item \textbf{Power Rule:} If $f(x) = x^n$, then $f'(x) = nx^{n-1}$.

                    Example:
                    \begin{align*}
                        f(x)  & = x^5             \\
                        f'(x) & = 5x^{5-1} = 5x^4
                    \end{align*}

              \item \textbf{Constant Multiple Rule:} If $f(x) = c \cdot g(x)$, then $f'(x) = c \cdot g'(x)$.

                    Example:
                    \begin{align*}
                        f(x)  & = 3x^2            \\
                        f'(x) & = 3 \cdot 2x = 6x
                    \end{align*}

              \item \textbf{Sum/Difference Rule:} If $f(x) = g(x) \pm h(x)$, then $f'(x) = g'(x) \pm h'(x)$.

                    Example:
                    \begin{align*}
                        f(x)  & = x^3 + 2x^2 - 5x + 7 \\
                        f'(x) & = 3x^2 + 4x - 5
                    \end{align*}

              \item \textbf{Product Rule:} If $f(x) = g(x) \cdot h(x)$, then $f'(x) = g'(x)h(x) + g(x)h'(x)$.

                    Example:
                    \begin{align*}
                        f(x)  & = (x^2 + 1)(x^3 - 2)              \\
                        g(x)  & = x^2 + 1, \quad g'(x) = 2x       \\
                        h(x)  & = x^3 - 2, \quad h'(x) = 3x^2     \\
                        f'(x) & = (2x)(x^3 - 2) + (x^2 + 1)(3x^2) \\
                              & = 2x^4 - 4x + 3x^4 + 3x^2         \\
                              & = 5x^4 + 3x^2 - 4x
                    \end{align*}

              \item \textbf{Quotient Rule:} If $f(x) = \frac{g(x)}{h(x)}$, then $f'(x) = \frac{g'(x)h(x) - g(x)h'(x)}{[h(x)]^2}$.

                    Example:
                    \begin{align*}
                        f(x)  & = \frac{x^2 + 1}{x - 3}                  \\
                        g(x)  & = x^2 + 1, \quad g'(x) = 2x              \\
                        h(x)  & = x - 3, \quad h'(x) = 1                 \\
                        f'(x) & = \frac{(2x)(x-3) - (x^2+1)(1)}{(x-3)^2} \\
                              & = \frac{2x^2 - 6x - x^2 - 1}{(x-3)^2}    \\
                              & = \frac{x^2 - 6x - 1}{(x-3)^2}
                    \end{align*}

              \item \textbf{Chain Rule:} If $f(x) = g(h(x))$, then $f'(x) = g'(h(x)) \cdot h'(x)$.

                    Example:
                    \begin{align*}
                        f(x)          & = (x^2 + 3x)^5                       \\
                        \text{Let } u & = x^2 + 3x, \text{ then } f(x) = u^5 \\
                        \frac{df}{du} & = 5u^4, \quad \frac{du}{dx} = 2x + 3 \\
                        f'(x)         & = \frac{df}{du} \cdot \frac{du}{dx}  \\
                                      & = 5(x^2 + 3x)^4 \cdot (2x + 3)
                    \end{align*}

              \item \textbf{Exponential Rule:} If $f(x) = e^{g(x)}$, then $f'(x) = e^{g(x)} \cdot g'(x)$.

                    Example:
                    \begin{align*}
                        f(x)  & = e^{3x^2 + 2x}                \\
                        f'(x) & = e^{3x^2 + 2x} \cdot (6x + 2) \\
                              & = (6x + 2)e^{3x^2 + 2x}
                    \end{align*}

              \item \textbf{Logarithmic Rule:} If $f(x) = \ln{g(x)}$, then $f'(x) = \frac{g'(x)}{g(x)}$.

                    Example:
                    \begin{align*}
                        f(x)  & = \ln{x^2 + 1}       \\
                        f'(x) & = \frac{2x}{x^2 + 1}
                    \end{align*}

              \item \textbf{Trigonometric Functions:}
                    \begin{itemize}
                        \item $\frac{d}{dx}\sin{x} = \cos{x}$
                        \item $\frac{d}{dx}\cos{x} = -\sin{x}$
                        \item $\frac{d}{dx}\tan{x} = \sec^2{x}$
                    \end{itemize}

                    Example:
                    \begin{align*}
                        f(x)  & = \sin{2x^2}                         \\
                        f'(x) & = \cos{2x^2} \cdot 4x = 4x\cos{2x^2}
                    \end{align*}

              \item \textbf{Implicit Differentiation:} A technique used to find $\frac{dy}{dx}$ when $y$ cannot be easily expressed as an explicit function of $x$. Instead of solving for $y$ first, we differentiate both sides of the equation with respect to $x$, treating $y$ as an implicit function of $x$, and then solve for $\frac{dy}{dx}$.

                    \textbf{Key Steps:}
                    \begin{enumerate}
                        \item Differentiate both sides of the equation with respect to $x$
                        \item Apply the chain rule to terms involving $y$: $\frac{d}{dx}[f(y)] = f'(y) \cdot \frac{dy}{dx}$
                        \item Collect all terms with $\frac{dy}{dx}$ on one side
                        \item Factor out $\frac{dy}{dx}$ and solve
                    \end{enumerate}

                    Example 1: Find $\frac{dy}{dx}$ for the circle $x^2 + y^2 = 25$.
                    \begin{align*}
                        x^2 + y^2                             & = 25                                         \\
                        \\
                        \text{Differentiate both sides with respect to } x\text{:}                           \\
                        \frac{d}{dx}(x^2 + y^2)               & = \frac{d}{dx}(25)                           \\
                        \frac{d}{dx}(x^2) + \frac{d}{dx}(y^2) & = 0                                          \\
                        2x + 2y\frac{dy}{dx}                  & = 0 \quad \text{(chain rule on } y^2\text{)} \\
                        \\
                        \text{Solve for } \frac{dy}{dx}\text{:}                                              \\
                        2y\frac{dy}{dx}                       & = -2x                                        \\
                        \frac{dy}{dx}                         & = -\frac{2x}{2y} = -\frac{x}{y}
                    \end{align*}

                    Example 2: Find $\frac{dy}{dx}$ for $x^3 + y^3 = 6xy$.
                    \begin{align*}
                        x^3 + y^3                & = 6xy                                                                 \\
                        \\
                        \text{Differentiate both sides:}                                                                 \\
                        \frac{d}{dx}(x^3 + y^3)  & = \frac{d}{dx}(6xy)                                                   \\
                        3x^2 + 3y^2\frac{dy}{dx} & = 6\left(x\frac{dy}{dx} + y\right) \quad \text{(product rule on RHS)} \\
                        3x^2 + 3y^2\frac{dy}{dx} & = 6x\frac{dy}{dx} + 6y
                    \end{align*}

                    \begin{align*}
                        \text{Collect } \frac{dy}{dx} \text{ terms:}                                                          \\
                        3y^2\frac{dy}{dx} - 6x\frac{dy}{dx} & = 6y - 3x^2                                                     \\
                        (3y^2 - 6x)\frac{dy}{dx}            & = 6y - 3x^2                                                     \\
                        \frac{dy}{dx}                       & = \frac{6y - 3x^2}{3y^2 - 6x} = \frac{3(2y - x^2)}{3(y^2 - 2x)} \\
                        \frac{dy}{dx}                       & = \frac{2y - x^2}{y^2 - 2x}
                    \end{align*}

                    Example 3: Find the slope of the tangent line to $\sin{xy} + x = y^2$ at point $(0, 0)$.
                    \begin{align*}
                        \sin{xy} + x                                       & = y^2                                                      \\
                        \\
                        \text{Differentiate:}                                                                                           \\
                        \frac{d}{dx}[\sin{xy}] + \frac{d}{dx}(x)           & = \frac{d}{dx}(y^2)                                        \\
                        \cos{xy} \cdot \frac{d}{dx}(xy) + 1                & = 2y\frac{dy}{dx}                                          \\
                        \cos{xy} \cdot \left(x\frac{dy}{dx} + y\right) + 1 & = 2y\frac{dy}{dx}                                          \\
                        x\cos{xy}\frac{dy}{dx} + y\cos{xy} + 1             & = 2y\frac{dy}{dx}                                          \\
                        \\
                        \text{Solve for } \frac{dy}{dx}\text{:}                                                                         \\
                        x\cos{xy}\frac{dy}{dx} - 2y\frac{dy}{dx}           & = -y\cos{xy} - 1                                           \\
                        [x\cos{xy} - 2y]\frac{dy}{dx}                      & = -y\cos{xy} - 1                                           \\
                        \frac{dy}{dx}                                      & = \frac{-y\cos{xy} - 1}{x\cos{xy} - 2y}                    \\
                        \\
                        \text{At point } (0, 0)\text{:}                                                                                 \\
                        \frac{dy}{dx}\bigg|_{(0,0)}                        & = \frac{-0 \cdot \cos{0} - 1}{0 \cdot \cos{0} - 2(0)}      \\
                                                                           & = \frac{-1}{0} \quad \text{(undefined - vertical tangent)}
                    \end{align*}

              \item \textbf{Mixed Partial Derivatives:} For a function of multiple variables $f(x, y)$, mixed partial derivatives involve taking partial derivatives with respect to different variables in succession. For most functions (those with continuous second derivatives), the order of differentiation doesn't matter, known as \textbf{Clairaut's Theorem} or \textbf{Schwarz's Theorem}:
                    \[
                        \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}
                    \]

                    \textbf{Notation:}
                    \begin{itemize}
                        \item $f_{xy} = \frac{\partial^2 f}{\partial y \partial x}$ (differentiate first with respect to $x$, then $y$)
                        \item $f_{yx} = \frac{\partial^2 f}{\partial x \partial y}$ (differentiate first with respect to $y$, then $x$)
                    \end{itemize}

                    Example 1: Find all second-order partial derivatives of $f(x, y) = x^3y^2 + 2xy$.
                    \begin{align*}
                        \text{First-order partial derivatives:}             &                                                         \\
                        f_x = \frac{\partial f}{\partial x}                 & = 3x^2y^2 + 2y                                          \\
                        f_y = \frac{\partial f}{\partial y}                 & = 2x^3y + 2x                                            \\
                        \\
                        \text{Second-order partial derivatives:}            &                                                         \\
                        f_{xx} = \frac{\partial^2 f}{\partial x^2}          & = \frac{\partial}{\partial x}(3x^2y^2 + 2y) = 6xy^2     \\
                        f_{yy} = \frac{\partial^2 f}{\partial y^2}          & = \frac{\partial}{\partial y}(2x^3y + 2x) = 2x^3        \\
                        \\
                        \text{Mixed partial derivatives:}                   &                                                         \\
                        f_{xy} = \frac{\partial^2 f}{\partial y \partial x} & = \frac{\partial}{\partial y}(3x^2y^2 + 2y) = 6x^2y + 2 \\
                        f_{yx} = \frac{\partial^2 f}{\partial x \partial y} & = \frac{\partial}{\partial x}(2x^3y + 2x) = 6x^2y + 2   \\
                        \\
                        \text{Note: } f_{xy}                                & = f_{yx} \text{ (Clairaut's Theorem verified)}
                    \end{align*}

                    Example 2: Find mixed partials for $f(x, y, z) = x^2yz + xe^{yz}$.
                    \begin{align*}
                        f_x    & = 2xyz + e^{yz}                                                            \\
                        f_y    & = x^2z + xze^{yz}                                                          \\
                        f_z    & = x^2y + xye^{yz}                                                          \\
                        \\
                        f_{xy} & = \frac{\partial}{\partial y}(2xyz + e^{yz}) = 2xz + ze^{yz}               \\
                        f_{yx} & = \frac{\partial}{\partial x}(x^2z + xze^{yz}) = 2xz + ze^{yz}             \\
                        \\
                        f_{xz} & = \frac{\partial}{\partial z}(2xyz + e^{yz}) = 2xy + ye^{yz}               \\
                        f_{zx} & = \frac{\partial}{\partial x}(x^2y + xye^{yz}) = 2xy + ye^{yz}             \\
                        \\
                        f_{yz} & = \frac{\partial}{\partial z}(x^2z + xze^{yz}) = x^2 + xze^{yz} + xye^{yz} \\
                        f_{zy} & = \frac{\partial}{\partial y}(x^2y + xye^{yz}) = x^2 + xze^{yz} + xye^{yz}
                    \end{align*}
          \end{enumerate}

          \paragraph[short]{Critical Points and Extrema:} Critical points are points where the derivative is zero or undefined. These points are candidates for local maxima, local minima, or saddle points.
          \textbf{Definition:} A critical point of $f(x)$ occurs at $x = c$ if:
          \begin{itemize}
              \item $f'(c) = 0$ (horizontal tangent), or
              \item $f'(c)$ is undefined (vertical tangent or cusp)
          \end{itemize}

          \textbf{Types of Extrema:}
          \begin{itemize}
              \item \textbf{Local Maximum:} $f(c) \geq f(x)$ for all $x$ near $c$
              \item \textbf{Local Minimum:} $f(c) \leq f(x)$ for all $x$ near $c$
              \item \textbf{Global Maximum:} $f(c) \geq f(x)$ for all $x$ in the domain
              \item \textbf{Global Minimum:} $f(c) \leq f(x)$ for all $x$ in the domain
          \end{itemize}

          \textbf{First Derivative Test:}
          \begin{itemize}
              \item If $f'(x)$ changes from positive to negative at $x = c$, then $f$ has a local maximum at $c$
              \item If $f'(x)$ changes from negative to positive at $x = c$, then $f$ has a local minimum at $c$
              \item If $f'(x)$ does not change sign at $x = c$, then $f$ has neither a max nor min at $c$
          \end{itemize}

          \textbf{Second Derivative Test:}
          \begin{itemize}
              \item If $f'(c) = 0$ and $f''(c) > 0$, then $f$ has a local minimum at $c$
              \item If $f'(c) = 0$ and $f''(c) < 0$, then $f$ has a local maximum at $c$
              \item If $f'(c) = 0$ and $f''(c) = 0$, the test is inconclusive (use first derivative test)
          \end{itemize}

          Example 1: Find and classify all critical points of $f(x) = x^3 - 6x^2 + 9x + 1$.
          \begin{align*}
              f(x)              & = x^3 - 6x^2 + 9x + 1                                        \\
              f'(x)             & = 3x^2 - 12x + 9                                             \\
              \\
              \text{Set } f'(x) & = 0\text{:}                                                  \\
              3x^2 - 12x + 9    & = 0                                                          \\
              3(x^2 - 4x + 3)   & = 0                                                          \\
              3(x - 1)(x - 3)   & = 0                                                          \\
              x                 & = 1, 3 \quad \text{(critical points)}                        \\
              \\
              f''(x)            & = 6x - 12                                                    \\
              \\
              \text{At } x = 1: &                                                              \\
              f''(1)            & = 6(1) - 12 = -6 < 0 \quad \Rightarrow \text{ local maximum} \\
              f(1)              & = 1 - 6 + 9 + 1 = 5                                          \\
              \\
              \text{At } x = 3: &                                                              \\
              f''(3)            & = 6(3) - 12 = 6 > 0 \quad \Rightarrow \text{ local minimum}  \\
              f(3)              & = 27 - 54 + 27 + 1 = 1                                       \\
              \\
              \therefore        & \text{ Local max at } (1, 5), \text{ local min at } (3, 1)
          \end{align*}

          Example 2: Find extrema of $f(x) = \frac{x^2}{x - 1}$ on the interval $[2, 4]$.
          \begin{align*}
              f(x)              & = \frac{x^2}{x - 1}                                                         \\
              \\
              \text{Using quotient rule:}                                                                     \\
              f'(x)             & = \frac{2x(x-1) - x^2(1)}{(x-1)^2}                                          \\
                                & = \frac{2x^2 - 2x - x^2}{(x-1)^2}                                           \\
                                & = \frac{x^2 - 2x}{(x-1)^2}                                                  \\
                                & = \frac{x(x - 2)}{(x-1)^2}                                                  \\
              \\
              \text{Set } f'(x) & = 0\text{:}                                                                 \\
              x(x - 2)          & = 0                                                                         \\
              x                 & = 0, 2                                                                      \\
              \\
              \text{In } [2, 4] & \text{, only } x = 2 \text{ is a critical point}                            \\
              \\
              \text{Evaluate at critical point and endpoints:}                                                \\
              f(2)              & = \frac{4}{1} = 4 \quad \text{(critical point)}                             \\
              f(4)              & = \frac{16}{3} \approx 5.33 \quad \text{(right endpoint)}                   \\
              \\
              \therefore        & \text{ Absolute min at } (2, 4), \text{ absolute max at } (4, \frac{16}{3})
          \end{align*}

          \paragraph[short]{Concavity and Inflection Points:} Concavity describes the direction a curve bends. The second derivative determines concavity.

          \textbf{Concavity:}
          \begin{itemize}
              \item \textbf{Concave Up:} If $f''(x) > 0$ on an interval, the graph curves upward (like $\cup$)
              \item \textbf{Concave Down:} If $f''(x) < 0$ on an interval, the graph curves downward (like $\cap$)
          \end{itemize}

          \textbf{Inflection Point:} A point where the concavity changes (from up to down or vice versa). Occurs where $f''(x) = 0$ or $f''(x)$ is undefined, AND the concavity changes.

          \textbf{Steps to Find Inflection Points:}
          \begin{enumerate}
              \item Find $f''(x)$
              \item Solve $f''(x) = 0$ and identify where $f''(x)$ is undefined
              \item Test intervals around these points to verify concavity change
              \item If concavity changes, it's an inflection point
          \end{enumerate}

          Example 1: Find intervals of concavity and inflection points for $f(x) = x^3 - 6x^2 + 9x + 1$.
          \begin{align*}
              f(x)               & = x^3 - 6x^2 + 9x + 1                                      \\
              f'(x)              & = 3x^2 - 12x + 9                                           \\
              f''(x)             & = 6x - 12                                                  \\
              \\
              \text{Set } f''(x) = 0\text{:}                                                  \\
              6x - 12            & = 0                                                        \\
              x                  & = 2 \quad \text{(potential inflection point)}              \\
              \\
              \text{Test intervals:}                                                          \\
              \text{For } x < 2: & \quad f''(1) = 6 - 12 = -6 < 0 \quad \text{(concave down)} \\
              \text{For } x > 2: & \quad f''(3) = 18 - 12 = 6 > 0 \quad \text{(concave up)}   \\
              \\
              \text{At } x = 2:  &                                                            \\
              f(2)               & = 8 - 24 + 18 + 1 = 3                                      \\
              \\
              \therefore         & \text{ Inflection point at } (2, 3)                        \\
                                 & \text{ Concave down on } (-\infty, 2)                      \\
                                 & \text{ Concave up on } (2, \infty)
          \end{align*}

          Example 2: Analyze $f(x) = x^4 - 4x^3$ for concavity and inflection points.
          \begin{align*}
              f(x)       & = x^4 - 4x^3                                               \\
              f'(x)      & = 4x^3 - 12x^2                                             \\
              f''(x)     & = 12x^2 - 24x                                              \\
                         & = 12x(x - 2)                                               \\
              \\
              \text{Set } f''(x) = 0\text{:}                                          \\
              12x(x - 2) & = 0                                                        \\
              x          & = 0, 2                                                     \\
              \\
              \text{Test intervals:}                                                  \\
              x < 0:     & \quad f''(-1) = 12 + 24 = 36 > 0 \quad \text{(up)}         \\
              0 < x < 2: & \quad f''(1) = 12 - 24 = -12 < 0 \quad \text{(down)}       \\
              x > 2:     & \quad f''(3) = 108 - 72 = 36 > 0 \quad \text{(up)}         \\
              \\
              f(0)       & = 0                                                        \\
              f(2)       & = 16 - 32 = -16                                            \\
              \\
              \therefore & \text{ Inflection points at } (0, 0) \text{ and } (2, -16) \\
                         & \text{ Concave up: } (-\infty, 0) \cup (2, \infty)         \\
                         & \text{ Concave down: } (0, 2)
          \end{align*}

          Example 3: Complete analysis of $f(x) = xe^{-x}$.
          \begin{align*}
              f(x)              & = xe^{-x}                                              \\
              \\
              \text{First derivative (product rule):}                                    \\
              f'(x)             & = (1)e^{-x} + x(-e^{-x})                               \\
                                & = e^{-x}(1 - x)                                        \\
              \\
              \text{Critical points: } f'(x) = 0                                         \\
              e^{-x}(1 - x)     & = 0                                                    \\
              x                 & = 1 \quad (e^{-x} \neq 0)                              \\
              \\
              \text{Second derivative:}                                                  \\
              f''(x)            & = -e^{-x}(1-x) + e^{-x}(-1)                            \\
                                & = e^{-x}[-(1-x) - 1]                                   \\
                                & = e^{-x}(x - 2)                                        \\
              \\
              \text{At } x = 1: &                                                        \\
              f''(1)            & = e^{-1}(-1) < 0 \quad \Rightarrow \text{ local max}   \\
              f(1)              & = e^{-1} \approx 0.368                                 \\
              \\
              \text{Inflection: } f''(x) = 0                                             \\
              e^{-x}(x - 2)     & = 0                                                    \\
              x                 & = 2                                                    \\
              f(2)              & = 2e^{-2} \approx 0.271                                \\
              \\
              \text{Concavity test:}                                                     \\
              x < 2:            & \quad f''(1) < 0 \quad \text{(concave down)}           \\
              x > 2:            & \quad f''(3) = e^{-3}(1) > 0 \quad \text{(concave up)} \\
              \\
              \therefore        & \text{ Local max at } (1, e^{-1})                      \\
                                & \text{ Inflection point at } (2, 2e^{-2})              \\
                                & \text{ Concave down on } (-\infty, 2)                  \\
                                & \text{ Concave up on } (2, \infty)
          \end{align*}
\end{itemize}

\paragraph[short]{Regression Analysis:} A supervised learning technique that models the relationship between one or more independent variables ($X$) and a dependent variable ($Y$). The goal is to find a function that best predicts continuous values based on input data by minimizing the prediction error.

\vspace{0.25cm}

\noindent \textbf{Simple Linear Regression:} Models the relationship using a straight line: $y = mx + c$, where:
\begin{itemize}
    \item $y$ = dependent variable (output/prediction)
    \item $x$ = independent variable (input/feature)
    \item $m$ = slope (rate of change of $y$ with respect to $x$)
    \item $c$ = y-intercept (value of $y$ when $x = 0$)
\end{itemize}

\pagebreak

\noindent Example: Predict house prices based on area.

\begin{center}
    \begin{tabular}{|l|l|}
        \hline
        House Area (sq. ft) & Price (taka) \\
        \hline
        1000                & 200000       \\
        \hline
        1500                & 250000       \\
        \hline
        2000                & 300000       \\
        \hline
        2200                & ?            \\
        \hline
    \end{tabular}
\end{center}

\noindent \textbf{Methods to Calculate Slope and Intercept:}

\begin{enumerate}
    \item \textbf{Ordinary Least Squares (OLS):} Minimizes the sum of squared residuals (vertical distances between actual and predicted values). This is the most common analytical solution.

          \textbf{Formulas:}
          \[
              m = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
          \]
          \[
              c = \bar{y} - m\bar{x}
          \]

          where $\bar{x} = \frac{1}{n}\sum x_i$ and $\bar{y} = \frac{1}{n}\sum y_i$ are the means.

          \textbf{Calculation for the example:}
          \begin{align*}
              n                               & = 3, \quad \bar{x} = \frac{1000 + 1500 + 2000}{3} = 1500                                                        \\
              \bar{y}                         & = \frac{200000 + 250000 + 300000}{3} = 250000                                                                   \\
              \\
              m                               & = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}                                              \\
                                              & = \frac{(1000-1500)(200000-250000) + (1500-1500)(250000-250000)}{(1000-1500)^2 + (1500-1500)^2 + (2000-1500)^2} \\
                                              & \quad + \frac{(2000-1500)(300000-250000)}{(1000-1500)^2 + (1500-1500)^2 + (2000-1500)^2}                        \\
                                              & = \frac{(-500)(-50000) + 0 + (500)(50000)}{250000 + 0 + 250000}                                                 \\
                                              & = \frac{25000000 + 25000000}{500000} = \frac{50000000}{500000} = 100                                            \\
              \\
              c                               & = \bar{y} - m\bar{x}                                                                                            \\
                                              & = 250000 - 100(1500) = 250000 - 150000 = 100000                                                                 \\
              \\
              \therefore y                    & = 100x + 100000                                                                                                 \\
              \\
              \text{For } x = 2200\text{: } y & = 100(2200) + 100000                                                                                            \\
                                              & = 220000 + 100000 = 320000
          \end{align*}

    \item \textbf{Normal Equation (Matrix Form):} For multiple linear regression with $n$ samples and $p$ features, we represent the problem as $y = X\beta + \epsilon$, where $X$ is the design matrix, $\beta$ is the parameter vector, and $\epsilon$ is the error term.

          \textbf{Solution:}
          \[
              \beta = (X^TX)^{-1}X^Ty
          \]

          For simple linear regression with intercept:
          \[
              X = \begin{bmatrix}
                  1      & x_1    \\
                  1      & x_2    \\
                  \vdots & \vdots \\
                  1      & x_n
              \end{bmatrix}, \quad
              \beta = \begin{bmatrix}
                  c \\ m
              \end{bmatrix}, \quad
              y = \begin{bmatrix}
                  y_1 \\ y_2 \\ \vdots \\ y_n
              \end{bmatrix}
          \]

          \textbf{Example calculation:}
          \begin{align*}
              X    & = \begin{bmatrix}
                           1 & 1000 \\
                           1 & 1500 \\
                           1 & 2000
                       \end{bmatrix}, \quad
              y = \begin{bmatrix}
                      200000 \\
                      250000 \\
                      300000
                  \end{bmatrix}            \\
              \\
              X^TX & = \begin{bmatrix}
                           1    & 1    & 1    \\
                           1000 & 1500 & 2000
                       \end{bmatrix}
              \begin{bmatrix}
                  1 & 1000 \\
                  1 & 1500 \\
                  1 & 2000
              \end{bmatrix}
              = \begin{bmatrix}
                    3    & 4500    \\
                    4500 & 7250000
                \end{bmatrix}              \\
              \\
              X^Ty & = \begin{bmatrix}
                           1    & 1    & 1    \\
                           1000 & 1500 & 2000
                       \end{bmatrix}
              \begin{bmatrix}
                  200000 \\
                  250000 \\
                  300000
              \end{bmatrix}
              = \begin{bmatrix}
                    750000 \\
                    1175000000
                \end{bmatrix}
          \end{align*}

          \begin{align*}
              (X^TX)^{-1}      & = \frac{1}{3(7250000) - 4500^2} \begin{bmatrix}
                                                                     7250000 & -4500 \\
                                                                     -4500   & 3
                                                                 \end{bmatrix}       \\
                               & = \frac{1}{1500000} \begin{bmatrix}
                                                         7250000 & -4500 \\
                                                         -4500   & 3
                                                     \end{bmatrix}                   \\
                               & = \begin{bmatrix}
                                       \frac{29}{6}    & \frac{-3}{1000}  \\
                                       \frac{-3}{1000} & \frac{1}{500000}
                                   \end{bmatrix}
              \\ \\
              (X^TX)^{-1} X^Ty & = \begin{bmatrix}
                                       \frac{29}{6}    & \frac{-3}{1000}  \\
                                       \frac{-3}{1000} & \frac{1}{500000}
                                   \end{bmatrix} \quad \begin{bmatrix}
                                                           750000 \\
                                                           1175000000
                                                       \end{bmatrix}                  \\
                               & = \begin{bmatrix}
                                       3625000 - 3525000 \\
                                       -2250 + 2350
                                   \end{bmatrix}                                   \\
                               & = \begin{bmatrix} 100000 \\ 100 \end{bmatrix} = \beta
          \end{align*}

    \item \textbf{Gradient Descent:} An iterative optimization algorithm that minimizes the cost function (mean squared error) by updating parameters in the direction of steepest descent.

          \vspace{0.25cm}

          \noindent \textbf{What is a Gradient?}

          \noindent \textbf{Gradient} is a vector of partial derivatives with respect to all the input parameters of a function, where each partial derivative represents the rate of change of the function with respect to that variable considering all the other parameters being stationary.

          \noindent Example:
          \begin{align*}
              \text{Say,}                                                           \\
              f(x, y)                    & = 2x + 3y
              \\ \\
              \text{Gradient, } \nabla f & = \begin{bmatrix}
                                                 \frac{\partial f}{\partial x} \\[0.2cm]
                                                 \frac{\partial f}{\partial y} \\
                                             \end{bmatrix}
          \end{align*}

          \pagebreak

          \begin{align*}
              \frac{\partial f}{\partial x} & = 2 \\
              \frac{\partial f}{\partial y} & = 3 \\
          \end{align*}

          \noindent The gradient above can be interpreted as:
          \begin{itemize}
              \item $\frac{\partial f}{\partial x}$: If $x$ increases by 1 unit the output of the function increases by 2 (holding $y$ constant). For example, $f(1, 2) = 2 + 6 = 8$ and $f(2, 2) = 4 + 6 = 10$. Here, $x$ increased by 1 unit (1 to 2) and the output increased by 2 ($10 - 8 = 2$).
              \item $\frac{\partial f}{\partial y}$: If $y$ increases by 1 unit the output of the function increases by 3 (holding $x$ constant).
              \item $\begin{bmatrix}
                            2 \\ 3
                        \end{bmatrix}$ is the direction of the steepest ascent of this function.
              \item The maximum rate of change of this function is $||\nabla f|| = \sqrt{2^2 + 3^2} = \sqrt{13}.$
          \end{itemize}

          \noindent \textbf{Gradient Descent intuition:}
          \begin{enumerate}
              \item Start with random parameter values.
              \item Calculate the gradient (the maximum ascent direction).
              \item Move a small step in the opposite direction (maximum descent).
              \item Repeat until we reach the minimum.
          \end{enumerate}

          The learning rate $\alpha$ controls the step size - too large and we might overshoot, too small and convergence is slow.

          \noindent \textbf{Gradient Descent Variants:} Based on the sample size to compute gradient, gradient descent can be of three types.
          \begin{itemize}
              \item \textbf{Batch Gradient Descent:} It is the vanilla gradient descent. Here every epoch the whole training dataset is used to compute gradient and update the weights. Batch gradient descent is computationally very expensive.
              \item \textbf{Mini-Batch Gradient Descent:} Instead of using the whole dataset at once, it is divided into multiple parts of same size (usually 16, 32, 64, 128, \dots) and weights are updated. Each epoch has multiple steps which depends on the batch size.
              \item \textbf{Stochastic Gradient Descent (SGD):} Unlike previous methods, here only one sample is used to update weights at a time. So, each epoch has $n$ steps where $n$ is the total number of samples.
          \end{itemize}
          In case of \textbf{Mini-Batch Gradient Descent} and \textbf{SGD}, the training dataset is randomly shuffled at the beginning of each epoch.

          \vspace{0.25cm}

          \textbf{Cost Function:}
          \[
              J(m, c) = \frac{1}{2n}\sum_{i=1}^{n}(y_i - (mx_i + c))^2
          \]

          \noindent \textbf{Note:} The $\frac{1}{2}$ factor is included for mathematical convenience. When we take the derivative of the squared term, we get a factor of 2 from the chain rule, which cancels with the $\frac{1}{2}$. This simplifies the gradient formulas. The standard MSE uses $\frac{1}{n}$, but for gradient descent, $\frac{1}{2n}$ is conventional because:
          \begin{itemize}
              \item It doesn't change the location of the minimum (just scales the function)
              \item The derivative of $(y - \hat{y})^2$ gives $2(y - \hat{y})$, and $2 \times \frac{1}{2} = 1$, eliminating the coefficient
              \item Results in cleaner gradient expressions
          \end{itemize}

          \textbf{Update Rules:}
          \begin{align*}
              m & := m - \alpha \frac{\partial J}{\partial m} = m - \alpha \frac{1}{n}\sum_{i=1}^{n}(mx_i + c - y_i)x_i \\
              c & := c - \alpha \frac{\partial J}{\partial c} = c - \alpha \frac{1}{n}\sum_{i=1}^{n}(mx_i + c - y_i)
          \end{align*}

          where $\alpha$ is the learning rate (e.g., 0.01).

          \noindent \textbf{Derivation of gradients:}
          \begin{align*}
              \frac{\partial J}{\partial m} & = \frac{\partial}{\partial m}\left[\frac{1}{2n}\sum_{i=1}^{n}(y_i - mx_i - c)^2\right] \\
                                            & = \frac{1}{2n}\sum_{i=1}^{n}2(y_i - mx_i - c)(-x_i)                                    \\
                                            & = \frac{1}{n}\sum_{i=1}^{n}(mx_i + c - y_i)x_i
          \end{align*}

          The factor of 2 from differentiation cancels with the $\frac{1}{2}$ in the cost function.

          \textbf{Algorithm:}
          \begin{enumerate}
              \item Initialize $m = 0, c = 0$
              \item Repeat until convergence:
                    \begin{itemize}
                        \item Compute predictions: $\hat{y}_i = mx_i + c$
                        \item Compute gradients using all training samples
                        \item Update $m$ and $c$ simultaneously
                    \end{itemize}
              \item Stop when $|J^{(t)} - J^{(t-1)}| < \epsilon$ (e.g., $\epsilon = 10^{-6}$)
          \end{enumerate}

          \textbf{Advantages:} Works well with large datasets, can handle online learning.

          \textbf{Disadvantages:} Requires choosing learning rate, may converge slowly.

    \item \textbf{Regularized Methods:} Add penalty terms to prevent overfitting in complex models.

          \textbf{Ridge Regression (L2):}
          \[
              J(m, c) = \sum_{i=1}^{n}(y_i - (mx_i + c))^2 + \lambda m^2
          \]
          Solution: $\beta = (X^TX + \lambda I)^{-1}X^Ty$

          \textbf{Lasso Regression (L1):}
          \[
              J(m, c) = \sum_{i=1}^{n}(y_i - (mx_i + c))^2 + \lambda |m|
          \]

          where $\lambda > 0$ is the regularization parameter. Larger $\lambda$ forces slope toward zero, preventing overfitting.
\end{enumerate}

\noindent \textbf{Comparison of Methods:}
\begin{itemize}
    \item \textbf{OLS/Normal Equation:} Best for small to medium datasets, provides exact solution, requires $(X^TX)^{-1}$ to exist
    \item \textbf{Gradient Descent:} Best for large datasets or when matrix inversion is expensive, flexible and scalable
    \item \textbf{Regularized Methods:} Best when dealing with multicollinearity or to prevent overfitting in high-dimensional data
\end{itemize}

\vspace{0.25cm}

\noindent \textbf{Loss Functions and Cost Functions:} These quantify how well a model's predictions match the actual values. Understanding them is crucial for training and evaluating regression models.

\vspace{0.25cm}

\noindent \textbf{Terminology:}
\begin{itemize}
    \item \textbf{Loss Function:} Measures the error for a \textit{single} data point: $L(y_i, \hat{y}_i)$
    \item \textbf{Cost Function:} Aggregates the loss over the \textit{entire} dataset: $J = \frac{1}{n}\sum_{i=1}^{n}L(y_i, \hat{y}_i)$
\end{itemize}

\noindent \textbf{Common Loss/Cost Functions for Regression:}

\begin{enumerate}
    \item \textbf{Mean Squared Error (MSE):} Most commonly used for linear regression. Penalizes larger errors more heavily due to squaring.

          \textbf{Loss:} $L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$

          \textbf{Cost:}
          \[
              \text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
          \]

          \textbf{Properties:}
          \begin{itemize}
              \item Differentiable everywhere (smooth optimization)
              \item Sensitive to outliers (large errors dominate)
              \item Units are squared (e.g., price$^2$)
              \item Convex function (guarantees global minimum)
          \end{itemize}

          Example: For predictions $\hat{y} = [100, 150, 200]$ and actual $y = [110, 145, 205]$:
          \begin{align*}
              \text{MSE} & = \frac{1}{3}[(110-100)^2 + (145-150)^2 + (205-200)^2] \\
                         & = \frac{1}{3}[100 + 25 + 25] = \frac{150}{3} = 50
          \end{align*}

    \item \textbf{Root Mean Squared Error (RMSE):} Square root of MSE, brings units back to original scale.

          \textbf{Formula:}
          \[
              \text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
          \]

          \textbf{Properties:}
          \begin{itemize}
              \item Same units as target variable (easier to interpret)
              \item Still sensitive to outliers
              \item Commonly reported metric
          \end{itemize}

          Example (using previous data): $\text{RMSE} = \sqrt{50} \approx 7.07$

    \item \textbf{Mean Absolute Error (MAE):} Uses absolute differences instead of squaring.

          \textbf{Loss:} $L(y_i, \hat{y}_i) = |y_i - \hat{y}_i|$

          \textbf{Cost:}
          \[
              \text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|
          \]

          \textbf{Properties:}
          \begin{itemize}
              \item More robust to outliers than MSE
              \item Same units as target variable
              \item Not differentiable at zero (optimization challenges)
              \item All errors weighted equally
          \end{itemize}

          Example: $\text{MAE} = \frac{1}{3}[|10| + |-5| + |5|] = \frac{20}{3} \approx 6.67$

    \item \textbf{Huber Loss:} Combines MSE and MAE - quadratic for small errors, linear for large errors.

          \textbf{Loss:}
          \[
              L_\delta(y_i, \hat{y}_i) = \begin{cases}
                  \frac{1}{2}(y_i - \hat{y}_i)^2                & \text{if } |y_i - \hat{y}_i| \leq \delta \\
                  \delta|y_i - \hat{y}_i| - \frac{1}{2}\delta^2 & \text{otherwise}
              \end{cases}
          \]

          where $\delta$ is a threshold parameter (typically 1.0).

          \textbf{Properties:}
          \begin{itemize}
              \item Less sensitive to outliers than MSE
              \item Differentiable everywhere (unlike MAE)
              \item Requires tuning $\delta$ parameter
              \item Good balance between MSE and MAE
          \end{itemize}

    \item \textbf{Log-Cosh Loss:} Logarithm of the hyperbolic cosine of prediction error.

          \textbf{Loss:} $L(y_i, \hat{y}_i) = \log{\cosh{y_i - \hat{y}_i}}$

          \textbf{Cost:}
          \[
              J = \frac{1}{n}\sum_{i=1}^{n}\log{\cosh{y_i - \hat{y}_i}}
          \]

          \textbf{Properties:}
          \begin{itemize}
              \item Approximately equal to MSE for small errors
              \item Approximately linear (like MAE) for large errors
              \item Differentiable everywhere
              \item Smooth and convex
          \end{itemize}

    \item \textbf{R Score (Coefficient of Determination):} Not a loss function, but a common evaluation metric. Measures proportion of variance explained by the model.

          \textbf{Formula:}
          \[
              R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} = 1 - \frac{\text{RSS}}{\text{TSS}}
          \]

          where RSS = Residual Sum of Squares, TSS = Total Sum of Squares.

          \textbf{Properties:}
          \begin{itemize}
              \item Range: $(-\infty, 1]$, where 1 is perfect fit
              \item $R^2 = 0$ means model performs as well as mean baseline
              \item Negative $R^2$ means model performs worse than mean
              \item Higher is better (unlike loss functions)
          \end{itemize}
\end{enumerate}

\noindent \textbf{Choosing a Loss Function:}
\begin{itemize}
    \item \textbf{MSE/RMSE:} Use when large errors are particularly undesirable and data has few outliers
    \item \textbf{MAE:} Use when outliers are present or all errors should be weighted equally
    \item \textbf{Huber/Log-Cosh:} Use for robust regression with moderate outliers
    \item \textbf{Consider domain:} Some fields prefer specific metrics (e.g., MAE for demand forecasting)
\end{itemize}

\paragraph[short]{Correlation in Regression:} Correlation measures the strength and direction of the linear relationship between two variables. The correlation coefficient, denoted by $r$ (Pearson's correlation coefficient), quantifies this relationship and ranges from $-1$ to $+1$.

\vspace{0.25cm}

\noindent \textbf{Interpretation of Correlation Coefficient (r) Ranges:}
\begin{itemize}
    \item \textbf{$r = +1$:} Perfect positive linear correlation. All points lie exactly on a line with positive slope. As $x$ increases, $y$ increases proportionally.

    \item \textbf{$+0.7 < r < +1$:} Strong positive correlation. Points cluster closely around an upward-sloping line. Strong tendency for $y$ to increase as $x$ increases.

    \item \textbf{$+0.3 < r \leq +0.7$:} Moderate positive correlation. Points show a general upward trend but with noticeable scatter. Moderate tendency for $y$ to increase with $x$.

    \item \textbf{$0 < r \leq +0.3$:} Weak positive correlation. Points show slight upward trend with substantial scatter. Weak relationship between variables.

    \item \textbf{$r = 0$:} No linear correlation. No linear relationship between variables (though non-linear relationships may exist).

    \item \textbf{$-0.3 < r < 0$:} Weak negative correlation. Points show slight downward trend with substantial scatter.

    \item \textbf{$-0.7 < r \leq -0.3$:} Moderate negative correlation. Points show a general downward trend but with noticeable scatter.

    \item \textbf{$-1 < r \leq -0.7$:} Strong negative correlation. Points cluster closely around a downward-sloping line. Strong tendency for $y$ to decrease as $x$ increases.

    \item \textbf{$r = -1$:} Perfect negative linear correlation. All points lie exactly on a line with negative slope. As $x$ increases, $y$ decreases proportionally.
\end{itemize}

\vspace{0.25cm}

\noindent \textbf{Types of Correlation:}

\begin{enumerate}
    \item \textbf{Pearson Correlation Coefficient (r):} Measures the \textit{linear} relationship between two continuous variables. Assumes normal distribution and is sensitive to outliers.

          \textbf{Formula:}
          \[
              r = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
          \]

          where $\text{Cov}(X, Y)$ is the covariance, $\sigma_X$ and $\sigma_Y$ are standard deviations.

          \textbf{Properties:}
          \begin{itemize}
              \item Range: $[-1, +1]$
              \item Measures only linear relationships
              \item Sensitive to outliers
              \item Assumes continuous, normally distributed data
          \end{itemize}

          \textbf{Example:} Calculate Pearson's $r$ for study hours ($x$) and exam scores ($y$):
          \begin{center}
              \begin{tabular}{|c|c|}
                  \hline
                  Hours ($x$) & Score ($y$) \\
                  \hline
                  2           & 65          \\
                  4           & 75          \\
                  6           & 85          \\
                  8           & 95          \\
                  \hline
              \end{tabular}
          \end{center}

          \begin{align*}
              n = 4, \quad \bar{x}               & = \frac{2+4+6+8}{4} = 5, \quad \bar{y} = \frac{65+75+85+95}{4} = 80                \\
              \\
              \sum(x_i - \bar{x})(y_i - \bar{y}) & = (2-5)(65-80) + (4-5)(75-80)                                                      \\
                                                 & \quad + (6-5)(85-80) + (8-5)(95-80)                                                \\
                                                 & = (-3)(-15) + (-1)(-5) + (1)(5) + (3)(15)                                          \\
                                                 & = 45 + 5 + 5 + 45 = 100                                                            \\
              \\
              \sum(x_i - \bar{x})^2              & = (-3)^2 + (-1)^2 + (1)^2 + (3)^2 = 9 + 1 + 1 + 9 = 20                             \\
              \\
              \sum(y_i - \bar{y})^2              & = (-15)^2 + (-5)^2 + (5)^2 + (15)^2                                                \\
                                                 & = 225 + 25 + 25 + 225 = 500                                                        \\
              \\
              r                                  & = \frac{100}{\sqrt{20}\sqrt{500}} = \frac{100}{\sqrt{10000}} = \frac{100}{100} = 1
          \end{align*}

          Perfect positive correlation: more study hours perfectly predict higher scores.

    \item \textbf{Spearman's Rank Correlation ($\rho$ or $r_s$):} Measures the \textit{monotonic} relationship (not necessarily linear) between two variables by using ranks instead of raw values. More robust to outliers than Pearson's $r$.

          \textbf{Formula:}
          \[
              \rho = 1 - \frac{6\sum_{i=1}^{n}d_i^2}{n(n^2 - 1)}
          \]

          where $d_i$ is the difference between ranks of corresponding values.

          \textbf{Properties:}
          \begin{itemize}
              \item Range: $[-1, +1]$
              \item Measures monotonic relationships (including non-linear)
              \item Robust to outliers
              \item Works with ordinal data
              \item No assumption of normality
          \end{itemize}

          \textbf{Example:} Calculate Spearman's $\rho$ for the data:
          \begin{center}
              \begin{tabular}{|c|c|c|c|c|}
                  \hline
                  $x$ & $y$ & Rank($x$) & Rank($y$) & $d = $ Rank($x$) - Rank($y$) \\
                  \hline
                  10  & 35  & 1         & 1         & 0                            \\
                  20  & 40  & 2         & 2         & 0                            \\
                  30  & 50  & 3         & 3         & 0                            \\
                  40  & 45  & 4         & 4         & 0                            \\
                  \hline
              \end{tabular}
          \end{center}

          \begin{align*}
              \sum d_i^2 & = 0^2 + 0^2 + 0^2 + 0^2 = 0            \\
              \\
              \rho       & = 1 - \frac{6(0)}{4(16-1)} = 1 - 0 = 1
          \end{align*}

    \item \textbf{Kendall's Tau ($\tau$):} Another rank-based correlation measure that counts concordant and discordant pairs. Better for small sample sizes.

          \textbf{Formula:}
          \[
              \tau = \frac{(\text{number of concordant pairs}) - (\text{number of discordant pairs})}{\frac{n(n-1)}{2}}
          \]

          or equivalently:
          \[
              \tau = \frac{C - D}{C + D}
          \]

          where $C$ = concordant pairs, $D$ = discordant pairs.

          \textbf{Properties:}
          \begin{itemize}
              \item Range: $[-1, +1]$
              \item More conservative than Spearman (typically lower values)
              \item Better for small sample sizes
              \item Interpretation in terms of probability of concordance
          \end{itemize}

    \item \textbf{Point-Biserial Correlation ($r_{pb}$):} Measures the relationship between a continuous variable and a binary (dichotomous) variable.

          \textbf{Formula:}
          \[
              r_{pb} = \frac{\bar{x}_1 - \bar{x}_0}{s_x}\sqrt{\frac{n_1 n_0}{n(n-1)}}
          \]

          where $\bar{x}_1$ and $\bar{x}_0$ are means for groups 1 and 0, $s_x$ is the standard deviation of the continuous variable, $n_1$ and $n_0$ are group sizes.

          \textbf{Use case:} Correlating exam scores (continuous) with pass/fail status (binary).

    \item \textbf{Partial Correlation:} Measures the relationship between two variables while controlling for the effect of one or more other variables.

          \textbf{Formula (controlling for variable $z$):}
          \[
              r_{xy \cdot z} = \frac{r_{xy} - r_{xz}r_{yz}}{\sqrt{(1 - r_{xz}^2)(1 - r_{yz}^2)}}
          \]

          where $r_{xy}$ is correlation between $x$ and $y$, $r_{xz}$ is correlation between $x$ and $z$, etc.

          \textbf{Use case:} Finding correlation between ice cream sales and drowning incidents while controlling for temperature.

    \item \textbf{Multiple Correlation ($R$):} Measures the strength of relationship between one dependent variable and multiple independent variables combined.

          \textbf{Formula (for two predictors):}
          \[
              R_{y \cdot x_1x_2} = \sqrt{\frac{r_{yx_1}^2 + r_{yx_2}^2 - 2r_{yx_1}r_{yx_2}r_{x_1x_2}}{1 - r_{x_1x_2}^2}}
          \]

          \textbf{Properties:}
          \begin{itemize}
              \item Range: $[0, +1]$ (always positive)
              \item Used in multiple regression analysis
              \item $R^2$ is the coefficient of determination
          \end{itemize}
\end{enumerate}

\vspace{0.25cm}

\noindent \textbf{Important Notes:}
\begin{itemize}
    \item \textbf{Correlation $\neq$ Causation:} A strong correlation does not imply that one variable causes changes in the other.
    \item \textbf{Outliers:} Can significantly affect Pearson's $r$; use Spearman's $\rho$ for robustness.
    \item \textbf{Non-linear Relationships:} Pearson's $r$ may be near zero even with strong non-linear relationships.
    \item \textbf{Sample Size:} Small samples may show strong correlations by chance; use significance testing.
\end{itemize}

\paragraph[short]{Improvements on Gradient Descent:}
\begin{itemize}
    \item \textbf{Gradient Descent with Momentum:} In case of Mini-Batch Gradient Descent and SGD, the frequent and small weight updates creates noise as the true maximum descent direction cannot be interpreted from a small size of sample. Also, there could be such dimensions where gradient is higher than others. This causes oscillation (change of direction back-n-forth). To counter both of the cases, a momentum term is added with current gradient. It acts like a memory of past gradients causing bigger steps where the direction is consistent and cancels out/reduces oscillation.
          \begin{align*}
              v_{t+1} & = \beta v_{t} + g_{t+1}    \\
              w_{t+1} & = w_{t} - \alpha v_{t+1} \\
          \end{align*}
          Here,
          \begin{itemize}
              \item $v$ is the velocity which starts at $\begin{bmatrix}0 & 0 & \dots & 0\end{bmatrix}_{1 \times p}^T$ ($p$ is the number of parameters).
              \item $\beta$ is the momentum coefficient. $0 \le \beta < 1$ (usually 0.9, 0.99 etc.)
              \item $g$ is the gradient.
              \item $w$ is the weights.
              \item $\alpha$ is the learning rate. 
          \end{itemize}
    This algorithm, prioritizes newer gradients over older ones. Older gradients decays exponentially ($v_{t+1} = g_{t+1} + \beta g_{t} + \beta^2 g_{t-1} + \beta^3 g_{t-2} + \cdots + \beta^{t} g_{1}$).

    \item {RMSProp:}
\end{itemize}

\paragraph[short]{Extras:}
\begin{itemize}
    \item \textbf{Logarithmic Properties:}
          \begin{itemize}\small
              \item \textbf{Product Rule:} $\operatorname{log}_{a}{bc} = \operatorname{log}_{a}{b} \times \operatorname{log}_{a}{c}$.
              \item \textbf{Quotient Rule:} $\operatorname{log}_{a}{\frac{b}{c}} = \operatorname{log}_{a}{b} - \operatorname{log}_{a}{c}$.
              \item \textbf{Power Rule:} $\operatorname{log}_{a}{b^c} = c\operatorname{log}_{a}{b}$.
              \item \textbf{Change of Base:} $\operatorname{log}_{a}{b} = \frac{\operatorname{log}_{x}{b}}{\operatorname{log}_{x}{a}}$.
              \item \textbf{Logarithm of 1:} $\operatorname{log}_{a}{1} = 0$.
              \item \textbf{Logarithm of Base:} $\operatorname{log}_{a}{a} = 1$.
          \end{itemize}

    \item \textbf{Properties of $e$ and Natural Logarithms $\operatorname{ln}x$:}
          \begin{itemize}
              \item The constant $e \approx 2.718$ is the base of natural logarithm. $\operatorname{ln}{x} = \operatorname{ln}_{e}{x}$.
              \item $\operatorname{ln}{e} = 1$.
              \item $e^{\operatorname{ln}x} = x$ and $\operatorname{ln}{e^x} = x$.
              \item \textbf{Derivative and Integral of $e^x$:} $\frac{d}{dx}e^x = e^x$ and $\int{e^x}dx = e^x + C$.
              \item \textbf{Derivative and Integral of $\operatorname{ln}x$:} $\frac{d}{dx} \operatorname{ln}{x} = \frac{1}{x}$ and $\int{\operatorname{ln}{x}}dx = x\operatorname{ln}{x} - x + C$.
          \end{itemize}
\end{itemize}

\end{document}
